{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy:1.16.2\n",
      "TensorFlow:1.13.1\n",
      "Keras:2.2.4-tf\n",
      "OpenAI Gym: 0.12.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "print(\"NumPy:{}\".format(np.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(123)\n",
    "print(\"TensorFlow:{}\".format(tf.__version__))\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "print(\"Keras:{}\".format(keras.__version__))\n",
    "\n",
    "import gym\n",
    "print('OpenAI Gym:',gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Environments in Gym version 0.12.1 : 833\n"
     ]
    }
   ],
   "source": [
    "all_env = list(gym.envs.registry.all())\n",
    "print('Total Environments in Gym version {} : {}'\n",
    "    .format(gym.__version__,len(all_env)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(Copy-v0)\n",
      "EnvSpec(RepeatCopy-v0)\n",
      "EnvSpec(ReversedAddition-v0)\n",
      "EnvSpec(ReversedAddition3-v0)\n",
      "EnvSpec(DuplicatedInput-v0)\n",
      "EnvSpec(Reverse-v0)\n",
      "EnvSpec(CartPole-v0)\n",
      "EnvSpec(CartPole-v1)\n",
      "EnvSpec(MountainCar-v0)\n",
      "EnvSpec(MountainCarContinuous-v0)\n",
      "EnvSpec(Pendulum-v0)\n",
      "EnvSpec(Acrobot-v1)\n",
      "EnvSpec(LunarLander-v2)\n",
      "EnvSpec(LunarLanderContinuous-v2)\n",
      "EnvSpec(BipedalWalker-v2)\n",
      "EnvSpec(BipedalWalkerHardcore-v2)\n",
      "EnvSpec(CarRacing-v0)\n",
      "EnvSpec(Blackjack-v0)\n",
      "EnvSpec(KellyCoinflip-v0)\n",
      "EnvSpec(KellyCoinflipGeneralized-v0)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for e in list(all_env):\n",
    "    print(e)\n",
    "    i += 1\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('MsPacman-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(9)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_height = 210\n",
    "n_width = 160\n",
    "n_depth = 3\n",
    "n_shape = [n_height,n_width,n_depth]\n",
    "n_inputs = n_height * n_width * n_depth\n",
    "env.frameskip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished at t 535 with score 130.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "frame_time = 1.0 / 15  # seconds\n",
    "n_episodes = 1\n",
    "\n",
    "for i_episode in range(n_episodes):\n",
    "    t=0\n",
    "    score=0\n",
    "    then = 0\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        now = time.time()\n",
    "        if frame_time < now - then:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            env.render()\n",
    "            then = now\n",
    "            t=t+1\n",
    "    print('Episode {} finished at t {} with score {}'.format(i_episode,\n",
    "                                                             t,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score 232.0, max 290.0, min 150.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "frame_time = 1.0 / 15  # seconds\n",
    "n_episodes = 10\n",
    "\n",
    "scores = []\n",
    "for i_episode in range(n_episodes):\n",
    "    t=0\n",
    "    score=0\n",
    "    then = 0\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        now = time.time()\n",
    "        if frame_time < now - then:\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            env.render()\n",
    "            then = now\n",
    "            t=t+1\n",
    "    scores.append(score)\n",
    "    #print(\"Episode {} finished at t {} with score {}\".format(i_episode,t,score))\n",
    "print('Average score {}, max {}, min {}'.format(np.mean(scores),\n",
    "                                          np.max(scores),\n",
    "                                          np.min(scores)\n",
    "                                         ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_q_nn(obs, env):\n",
    "    # Exploration strategy - Select a random action\n",
    "    if np.random.random() < explore_rate:\n",
    "        action = env.action_space.sample()\n",
    "    # Exploitation strategy - Select the action with the highest q\n",
    "    else:\n",
    "        action = np.argmax(q_nn.predict(np.array([obs])))\n",
    "    return action\n",
    "\n",
    "def episode(env, policy, r_max=0, t_max=0):\n",
    "\n",
    "    # create the empty list to contain game memory\n",
    "    #memory = deque(maxlen=1000)\n",
    "    \n",
    "    # observe initial state\n",
    "    obs = env.reset()\n",
    "    state_prev = obs\n",
    "    #state_prev = np.ravel(obs) # replaced with keras reshape[-1]\n",
    "    \n",
    "    # initialize the variables\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = policy(state_prev, env)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        state_next = obs\n",
    "        #state_next = np.ravel(obs) # replaced with keras reshape[-1]\n",
    "        \n",
    "                                             \n",
    "        # add the state_prev, action, reward, state_new, done to memory\n",
    "        memory.append([state_prev,action,reward,state_next,done])\n",
    "                           \n",
    "        \n",
    "        # Generate and update the q_values with \n",
    "        # maximum future rewards using bellman function:\n",
    "        states = np.array([x[0] for x in memory])\n",
    "        states_next = np.array([np.zeros(n_shape) if x[4] else x[3] for x in memory])\n",
    "        \n",
    "        q_values = q_nn.predict(states)\n",
    "        q_values_next = q_nn.predict(states_next)\n",
    "        \n",
    "        for i in range(len(memory)):\n",
    "            state_prev,action,reward,state_next,done = memory[i]\n",
    "            if done:\n",
    "                q_values[i,action] = reward\n",
    "            else:\n",
    "                best_q = np.amax(q_values_next[i])\n",
    "                bellman_q = reward + discount_rate * best_q\n",
    "                q_values[i,action] = bellman_q\n",
    "        \n",
    "        # train the q_nn with states and q_values, same as updating the q_table\n",
    "        q_nn.fit(states,q_values,epochs=1,batch_size=50,verbose=0)\n",
    "    \n",
    "        state_prev = state_next\n",
    "        \n",
    "        episode_reward += reward\n",
    "        if r_max > 0 and episode_reward > r_max:\n",
    "            break\n",
    "        t+=1\n",
    "        if t_max > 0 and t == t_max:\n",
    "            break\n",
    "    return episode_reward\n",
    "\n",
    "# experiment collect observations and rewards for each episode\n",
    "def experiment(env, policy, n_episodes,r_max=0, t_max=0):\n",
    "    \n",
    "    rewards=np.empty(shape=[n_episodes])\n",
    "    for i in range(n_episodes):\n",
    "        val = episode(env, policy, r_max, t_max)\n",
    "        #print('episode:{}, reward {}'.format(i,val))\n",
    "        rewards[i]=val\n",
    "            \n",
    "    print('Policy:{}, Min reward:{}, Max reward:{}, Average reward:{}'\n",
    "        .format(policy.__name__,\n",
    "              np.min(rewards),\n",
    "              np.max(rewards),\n",
    "              np.mean(rewards)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 100800)            0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 512)               51610112  \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 9)                 4617      \n",
      "=================================================================\n",
      "Total params: 51,614,729\n",
      "Trainable params: 51,614,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from collections import deque \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# build the Q-Network\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = n_shape))\n",
    "model.add(Dense(512, activation='relu',name='hidden1'))\n",
    "model.add(Dense(9, activation='softmax', name='output'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "model.summary()\n",
    "q_nn = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Policy:policy_q_nn, Min reward:520.0, Max reward:520.0, Average reward:520.0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "discount_rate = 0.9\n",
    "explore_rate = 0.2\n",
    "n_episodes = 1\n",
    "\n",
    "# create the empty list to contain game memory\n",
    "memory = deque(maxlen=1000)\n",
    "\n",
    "experiment(env, policy_q_nn, n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:policy_q_nn, Min reward:270.0, Max reward:280.0, Average reward:275.0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "discount_rate = 0.9\n",
    "explore_rate = 0.2\n",
    "n_episodes = 2\n",
    "\n",
    "# create the empty list to contain game memory\n",
    "memory = deque(maxlen=1000)\n",
    "\n",
    "experiment(env, policy_q_nn, n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 206, 156, 16)      1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 103, 78, 16)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128544)            0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 512)               65815040  \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 9)                 4617      \n",
      "=================================================================\n",
      "Total params: 65,820,873\n",
      "Trainable params: 65,820,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from collections import deque \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# build the CNN Q-Network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), \n",
    "                 strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=n_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu',name='hidden1'))\n",
    "model.add(Dense(9, activation='softmax', name='output'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "model.summary()\n",
    "q_nn = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:policy_q_nn, Min reward:200.0, Max reward:410.0, Average reward:305.0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "discount_rate = 0.9\n",
    "explore_rate = 0.2\n",
    "n_episodes = 2\n",
    "\n",
    "# create the empty list to contain game memory\n",
    "memory = deque(maxlen=1000)\n",
    "\n",
    "experiment(env, policy_q_nn, n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
