{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data from here: http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset#Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(),\"datasetslib\", \"data\")\n",
    "NUM_CLASSES = 43\n",
    "IMG_SIZE = 32\n",
    "\n",
    "#Training Parameters\n",
    "BATCH_SIZE =128\n",
    "EPOCHS =1000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Inference Parameters\n",
    "NUM_MONTE_CARLO = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "from skimage import color, exposure, transform\n",
    "from skimage import io\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from absl import flags\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.backends import backend_agg\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def extract_dataset():\n",
    "    # Extracting training data\n",
    "    if not os.path.exists(os.path.join(DATA_DIR, \"GTSRB\")):\n",
    "        zip_ref = zipfile.ZipFile(os.path.join(DATA_DIR, 'GTSRB_Final_Training_Images.zip'), 'r')\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "        zip_ref.close()\n",
    "        # Extracting Testing Data\n",
    "        zip_ref = zipfile.ZipFile(os.path.join(DATA_DIR, 'GTSRB_Final_Test_Images.zip'), 'r')\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "        zip_ref.close()\n",
    "        # Extracting the csv file which contains annotations\n",
    "        zip_ref = zipfile.ZipFile(os.path.join(DATA_DIR,'GTSRB_Final_Test_GT.zip'), 'r')\n",
    "        zip_ref.extractall(os.path.join(DATA_DIR,\"GTSRB\"))\n",
    "    else:\n",
    "        print (\"Data is already extracted in a folder. No need to extract from zipfile again\")\n",
    "\n",
    "def normalize_and_reshape_img(img):\n",
    "    # Histogram normalization in v channel\n",
    "    hsv = color.rgb2hsv(img)\n",
    "    hsv[:, :, 2] = exposure.equalize_hist(hsv[:, :, 2])\n",
    "    img = color.hsv2rgb(hsv)\n",
    "\n",
    "    # Crop of the centre\n",
    "    min_side = min(img.shape[:-1])\n",
    "    centre = img.shape[0] // 2, img.shape[1] // 2\n",
    "    img = img[centre[0] - min_side // 2:centre[0] + min_side // 2,\n",
    "              centre[1] - min_side // 2:centre[1] + min_side // 2,\n",
    "              :]\n",
    "    # Rescale to the desired size\n",
    "    img = transform.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    return img\n",
    "\n",
    "def get_class(img_path):\n",
    "    try:\n",
    "        return int(img_path.split('/')[-2])\n",
    "    except:\n",
    "        return int(img_path.split('\\\\')[-2])\n",
    "\n",
    "def preprocess_and_save_data(data_type ='train'):\n",
    "    '''\n",
    "    Preprocesses image data and saves the image features and labels as pickle files to be used for the model\n",
    "    :param data_type: data_type is 'train' or 'test'\n",
    "    :return: None\n",
    "    '''\n",
    "    if data_type =='train':\n",
    "        root_dir = os.path.join(DATA_DIR, 'GTSRB/Final_Training/Images/')\n",
    "        imgs = []\n",
    "        labels = []\n",
    "\n",
    "        all_img_paths = glob.glob(os.path.join(root_dir, '*/*.ppm'))\n",
    "        np.random.shuffle(all_img_paths)\n",
    "        for img_path in all_img_paths:\n",
    "            img = normalize_and_reshape_img(io.imread(img_path))\n",
    "            label = get_class(img_path)\n",
    "            imgs.append(img)\n",
    "            labels.append(label)\n",
    "        X_train = np.array(imgs, dtype='float32')\n",
    "        # Make one hot targets\n",
    "        Y_train = np.array(labels, dtype = 'uint8')\n",
    "\n",
    "        train_data = {\"features\": X_train, \"labels\": Y_train}\n",
    "        if not os.path.exists(os.path.join(DATA_DIR,\"Preprocessed_Data\")):\n",
    "            os.makedirs(os.path.join(DATA_DIR,\"Preprocessed_Data\"))\n",
    "        pickle.dump(train_data,open(os.path.join(DATA_DIR,\"Preprocessed_Data\",\"preprocessed_train.p\"),\"wb\"))\n",
    "        return train_data\n",
    "    elif data_type == 'test':\n",
    "        # Reading the test file\n",
    "        test = pd.read_csv(os.path.join(DATA_DIR, \"GTSRB\", 'GT-final_test.csv'), sep=';')\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        i = 0\n",
    "        for file_name, class_id in zip(list(test['Filename']), list(test['ClassId'])):\n",
    "            img_path = os.path.join(DATA_DIR, 'GTSRB/Final_Test/Images/', file_name)\n",
    "            X_test.append(normalize_and_reshape_img(io.imread(img_path)))\n",
    "            y_test.append(class_id)\n",
    "\n",
    "        test_data = {\"features\": np.array(X_test,dtype ='float32'), \"labels\": np.array(y_test,dtype = 'uint8')}\n",
    "        if not os.path.exists(os.path.join(DATA_DIR,\"Preprocessed_Data\")):\n",
    "            os.makedirs(os.path.join(DATA_DIR,\"Preprocessed_Data\"))\n",
    "        pickle.dump(test_data,open(os.path.join(DATA_DIR,\"Preprocessed_Data\",\"preprocessed_test.p\"),\"wb\"))\n",
    "        return test_data\n",
    "\n",
    "def load_preprocessed_data():\n",
    "\n",
    "    '''\n",
    "    Load the preprocessed data if already present. Else, preprocess the data and then load\n",
    "    :return:\n",
    "    '''\n",
    "    print (\"Loading the training data\")\n",
    "    if not os.path.isfile(os.path.join(DATA_DIR, \"Preprocessed_Data\", \"preprocessed_train.p\")):\n",
    "        print (\"Processed File doesn't exist. Preprocessing data first\")\n",
    "        train_data = preprocess_and_save_data(data_type='train')\n",
    "    else:\n",
    "        train_data= pickle.load(open(os.path.join(DATA_DIR,\"Preprocessed_Data\",\"preprocessed_train.p\"),\"rb\"))\n",
    "    X_train, y_train = train_data['features'], train_data['labels']\n",
    "\n",
    "    print (\"Loading the testing data\")\n",
    "    if not os.path.isfile(os.path.join(DATA_DIR, \"Preprocessed_Data\", \"preprocessed_test.p\")):\n",
    "        print (\"Processed File doesn't exist. Preprocessing data first\")\n",
    "        test_data = preprocess_and_save_data(data_type='test')\n",
    "    else:\n",
    "        test_data= pickle.load(open(os.path.join(DATA_DIR,\"Preprocessed_Data\",\"preprocessed_test.p\"),\"rb\"))\n",
    "    X_test, y_test = test_data['features'], test_data['labels']\n",
    "\n",
    "    return X_train, y_train, X_test,y_test\n",
    "\n",
    "\n",
    "def convert_to_grayscale(data):\n",
    "    data_gray = np.zeros((data.shape[0], data.shape[1], data.shape[2], 1))\n",
    "    for i in range(len(data)):\n",
    "        if i % 10000 == 0:\n",
    "            print(\"Num images converted to grayscale \", i)\n",
    "        temp = color.rgb2gray(data[i])\n",
    "        temp = temp.reshape((temp.shape[0], temp.shape[1], 1))\n",
    "        data_gray[i] = temp\n",
    "    return data_gray.astype(np.float32)\n",
    "\n",
    "def load_grayscale_images(data, data_type = 'train'):\n",
    "    '''\n",
    "    Converts the data to grayscale as we only care of classification and not the color of the traffic sign for now\n",
    "    :param data: image data to be converted to grayscale\n",
    "    :return: grayscale image/images\n",
    "    '''\n",
    "    if data_type == 'train':\n",
    "        if not os.path.exists(os.path.join(DATA_DIR, \"Preprocessed_Data\", 'preprocessed_train_gray.p')):\n",
    "            data_gray = convert_to_grayscale(data)\n",
    "            # Saving the data_gray as pickle\n",
    "            pickle.dump(data_gray, open(os.path.join(DATA_DIR, \"Preprocessed_Data\", \"preprocessed_train_gray.p\"), \"wb\"))\n",
    "        else:\n",
    "            data_gray = pickle.load(open(os.path.join(DATA_DIR, \"Preprocessed_Data\", \"preprocessed_train_gray.p\"), \"rb\"))\n",
    "    elif data_type == 'test':\n",
    "        if not os.path.exists(os.path.join(DATA_DIR, \"Preprocessed_Data\", 'preprocessed_test_gray.p')):\n",
    "            data_gray = convert_to_grayscale(data)\n",
    "            # Saving the data_gray as pickle\n",
    "            pickle.dump(data_gray, open(os.path.join(DATA_DIR, \"Preprocessed_Data\", \"preprocessed_test_gray.p\"), \"wb\"))\n",
    "        else:\n",
    "            data_gray = pickle.load(open(os.path.join(DATA_DIR, \"Preprocessed_Data\", \"preprocessed_test_gray.p\"), \"rb\"))\n",
    "\n",
    "    return data_gray\n",
    "\n",
    "\n",
    "def build_data_pipeline(X_train, X_test,y_train, y_test):\n",
    "    '''\n",
    "    Dataset iterator for training the model\n",
    "    :param X_train: Numpy array consisting of train images\n",
    "    :param X_test: Numpy array consisting of test images\n",
    "    :param y_train: Numpy array containing train labels\n",
    "    :param y_test: Numpy arrray containing test labels\n",
    "    :return: iterators for train and test\n",
    "    '''\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((np.float32(X_train), np.int32(y_train)))\n",
    "    train_batches = train_data.shuffle(50000, reshuffle_each_iteration=True).repeat().batch(BATCH_SIZE)\n",
    "    train_iterator = train_batches.make_one_shot_iterator()\n",
    "\n",
    "    # Building an iterator with test_dataset with batch_size = X_test.shape[0]. We use entire testing data for one shot iterator\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((np.float32(X_test),np.int32(y_test)))\n",
    "    test_frozen = (test_data.take(X_test.shape[0]).repeat().batch(X_test.shape[0]))\n",
    "    test_iterator = test_frozen.make_one_shot_iterator()\n",
    "\n",
    "    # Combine these into a feedable iterator that can switch between training\n",
    "    # and validation inputs.\n",
    "    iter_handle = tf.placeholder(tf.string, shape=[])\n",
    "    iterator_feed = tf.data.Iterator.from_string_handle(iter_handle, train_batches.output_types, train_batches.output_shapes)\n",
    "    images, labels = iterator_feed.get_next()\n",
    "\n",
    "    return images, labels, iter_handle, train_iterator, test_iterator\n",
    "\n",
    "\n",
    "\n",
    "def plot_input_data(X_train,y_train):\n",
    "    # Let's plot the images of a particular sign and see the differences\n",
    "    num_rows = 9\n",
    "    num_cols = 5\n",
    "\n",
    "    fig = plt.figure(figsize=(num_cols, num_rows))\n",
    "    gs = gridspec.GridSpec(num_rows, num_cols, wspace=0.0)\n",
    "    ax = [plt.subplot(gs[i]) for i in range(num_rows * num_cols)]\n",
    "    for i in range(num_rows * num_cols):\n",
    "        ax[i].axis('off')\n",
    "        if i < 43:\n",
    "            indexes = list(np.where(y_train == i))[0]\n",
    "            image = X_train[random.choice(indexes)]\n",
    "            ax[i].imshow(image, interpolation='nearest')\n",
    "\n",
    "    image_name = 'Input_Images.png'\n",
    "    if not os.path.exists(os.path.join(DATA_DIR, \"..\", \"plots\")):\n",
    "        os.makedirs(os.path.join(DATA_DIR, \"..\", \"plots\"))\n",
    "    fig.savefig(os.path.join(DATA_DIR, \"..\", \"plots\",image_name), dpi=fig.dpi)\n",
    "    plt.clf()\n",
    "\n",
    "def plot_weight_posteriors(names, qm_vals, qs_vals, fname):\n",
    "    \"\"\"Save a PNG plot with histograms of weight means and stddevs.\n",
    "    Args:\n",
    "        names: A Python `iterable` of `str` variable names.\n",
    "        qm_vals: A Python `iterable`, the same length as `names`,\n",
    "          whose elements are Numpy `array`s, of any shape, containing\n",
    "          posterior means of weight variables.\n",
    "        qs_vals: A Python `iterable`, the same length as `names`,\n",
    "          whose elements are Numpy `array`s, of any shape, containing\n",
    "          posterior standard deviations of weight varibles.\n",
    "        fname: Python `str` filename to save the plot to.\n",
    "    \"\"\"\n",
    "    fig = figure.Figure(figsize=(6, 3))\n",
    "    canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    for n, qm in zip(names, qm_vals):\n",
    "        sns.distplot(qm.flatten(), ax=ax, label=n)\n",
    "    ax.set_title(\"Mean of Weight\")\n",
    "    ax.set_xlim([-1.5, 1.5])\n",
    "    ax.legend()\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    for n, qs in zip(names, qs_vals):\n",
    "        sns.distplot(qs.flatten(), ax=ax)\n",
    "    ax.set_title(\"Stdev of Weights\")\n",
    "    ax.set_xlim([0, 1.])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    save_dir = os.path.join(DATA_DIR, \"..\",\"plots\")\n",
    "    canvas.print_figure(os.path.join(save_dir, fname), format=\"png\")\n",
    "    print(\"saved {}\".format(fname))\n",
    "\n",
    "\n",
    "def plot_heldout_prediction(input_vals, probs,\n",
    "                            fname,  title=\"\"):\n",
    "    \"\"\"Plotting uncertainity in prediction of a sampled image .\n",
    "    Args:\n",
    "        input_vals: A `float`-like Numpy `array` of shape\n",
    "          IMAGE_SHAPE`, containing a sampled test image.\n",
    "        probs: A `float`-like Numpy array of shape `[num_monte_carlo,\n",
    "          1, num_classes]` containing Monte Carlo samples of\n",
    "          class probabilities for a test image.\n",
    "        fname: Python `str` filename to save the plot to.\n",
    "        title: Python `str` title for the plot.\n",
    "    \"\"\"\n",
    "    save_dir = os.path.join(DATA_DIR, \"..\", \"plots\")\n",
    "    fig = figure.Figure(figsize=(1, 1))\n",
    "    canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.imshow(input_vals.reshape((IMG_SIZE,IMG_SIZE)), interpolation=\"None\")\n",
    "    canvas.print_figure(os.path.join(save_dir, fname + \"_image.png\"), format=\"png\")\n",
    "\n",
    "    fig = figure.Figure(figsize=(10, 5))\n",
    "    canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    #Predictions\n",
    "    y_pred_list = list(np.argmax(probs,axis=1).astype(np.int32))\n",
    "    bin_range = [x for x in range(43)]\n",
    "    ax.hist(y_pred_list,bins = bin_range)\n",
    "    ax.set_xticks(bin_range)\n",
    "    ax.set_title(\"Histogram of predicted class: \" + title)\n",
    "    ax.set_xlabel(\"Class\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    fig.tight_layout()\n",
    "    save_dir = os.path.join(DATA_DIR, \"..\", \"plots\")\n",
    "    canvas.print_figure(os.path.join(save_dir, fname + \"_predicted_class.png\"), format=\"png\")\n",
    "    print(\"saved {}\".format(fname))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Bayesian NN & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset from zip files\n",
      "Data is already extracted in a folder. No need to extract from zipfile again\n",
      "Loading the training data\n",
      "Loading the testing data\n",
      "Shape of X_train is  (39209, 32, 32, 3)\n",
      "Shape of X_test is  (12630, 32, 32, 3)\n",
      "Shape of y_train is  (39209,)\n",
      "Shape of y_test is  (12630,)\n",
      "Shape of X_train Grayscale is  (39209, 32, 32, 1)\n",
      "Shape of X_test is Grayscale (12630, 32, 32, 1)\n",
      "Preprocessing Done\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch:   0 Loss: 27.534 Accuracy: 0.031\n",
      "Epoch:   5 Loss: 27.352 Accuracy: 0.030\n",
      "Epoch:  10 Loss: 27.283 Accuracy: 0.033\n",
      "Epoch:  15 Loss: 27.241 Accuracy: 0.036\n",
      "Epoch:  20 Loss: 27.128 Accuracy: 0.042\n",
      "Epoch:  25 Loss: 27.061 Accuracy: 0.046\n",
      "Epoch:  30 Loss: 26.970 Accuracy: 0.049\n",
      "Epoch:  35 Loss: 27.011 Accuracy: 0.050\n",
      "Epoch:  40 Loss: 26.903 Accuracy: 0.051\n",
      "Epoch:  45 Loss: 26.835 Accuracy: 0.054\n",
      "Epoch:  50 Loss: 26.852 Accuracy: 0.054\n",
      "Epoch:  55 Loss: 26.739 Accuracy: 0.054\n",
      "Epoch:  60 Loss: 26.795 Accuracy: 0.055\n",
      "Epoch:  65 Loss: 26.813 Accuracy: 0.058\n",
      "Epoch:  70 Loss: 26.634 Accuracy: 0.060\n",
      "Epoch:  75 Loss: 26.630 Accuracy: 0.062\n",
      "Epoch:  80 Loss: 26.412 Accuracy: 0.066\n",
      "Epoch:  85 Loss: 26.414 Accuracy: 0.069\n",
      "Epoch:  90 Loss: 26.395 Accuracy: 0.074\n",
      "Epoch:  95 Loss: 26.052 Accuracy: 0.076\n",
      "Epoch: 100 Loss: 25.848 Accuracy: 0.080\n",
      "Epoch: 105 Loss: 26.147 Accuracy: 0.084\n",
      "Epoch: 110 Loss: 25.844 Accuracy: 0.090\n",
      "Epoch: 115 Loss: 25.705 Accuracy: 0.096\n",
      "Epoch: 120 Loss: 25.111 Accuracy: 0.104\n",
      "Epoch: 125 Loss: 25.567 Accuracy: 0.111\n",
      "Epoch: 130 Loss: 25.247 Accuracy: 0.119\n",
      "Epoch: 135 Loss: 25.071 Accuracy: 0.126\n",
      "Epoch: 140 Loss: 24.914 Accuracy: 0.133\n",
      "Epoch: 145 Loss: 25.089 Accuracy: 0.140\n",
      "Epoch: 150 Loss: 24.958 Accuracy: 0.148\n",
      "Epoch: 155 Loss: 24.770 Accuracy: 0.156\n",
      "Epoch: 160 Loss: 24.675 Accuracy: 0.163\n",
      "Epoch: 165 Loss: 24.551 Accuracy: 0.169\n",
      "Epoch: 170 Loss: 24.624 Accuracy: 0.176\n",
      "Epoch: 175 Loss: 24.239 Accuracy: 0.184\n",
      "Epoch: 180 Loss: 24.281 Accuracy: 0.192\n",
      "Epoch: 185 Loss: 24.203 Accuracy: 0.198\n",
      "Epoch: 190 Loss: 23.962 Accuracy: 0.204\n",
      "Epoch: 195 Loss: 24.291 Accuracy: 0.211\n",
      "Epoch: 200 Loss: 23.901 Accuracy: 0.216\n",
      "Epoch: 205 Loss: 24.143 Accuracy: 0.222\n",
      "Epoch: 210 Loss: 24.147 Accuracy: 0.227\n",
      "Epoch: 215 Loss: 23.809 Accuracy: 0.234\n",
      "Epoch: 220 Loss: 23.793 Accuracy: 0.240\n",
      "Epoch: 225 Loss: 23.578 Accuracy: 0.246\n",
      "Epoch: 230 Loss: 23.996 Accuracy: 0.252\n",
      "Epoch: 235 Loss: 23.855 Accuracy: 0.258\n",
      "Epoch: 240 Loss: 23.622 Accuracy: 0.264\n",
      "Epoch: 245 Loss: 23.907 Accuracy: 0.269\n",
      "Epoch: 250 Loss: 23.593 Accuracy: 0.275\n",
      "Epoch: 255 Loss: 23.246 Accuracy: 0.279\n",
      "Epoch: 260 Loss: 23.359 Accuracy: 0.285\n",
      "Epoch: 265 Loss: 23.339 Accuracy: 0.291\n",
      "Epoch: 270 Loss: 23.227 Accuracy: 0.296\n",
      "Epoch: 275 Loss: 23.492 Accuracy: 0.302\n",
      "Epoch: 280 Loss: 23.057 Accuracy: 0.307\n",
      "Epoch: 285 Loss: 23.035 Accuracy: 0.312\n",
      "Epoch: 290 Loss: 22.889 Accuracy: 0.318\n",
      "Epoch: 295 Loss: 23.214 Accuracy: 0.323\n",
      "Epoch: 300 Loss: 22.872 Accuracy: 0.328\n",
      "Epoch: 305 Loss: 22.595 Accuracy: 0.332\n",
      "Epoch: 310 Loss: 22.898 Accuracy: 0.337\n",
      "Epoch: 315 Loss: 22.742 Accuracy: 0.341\n",
      "Epoch: 320 Loss: 22.698 Accuracy: 0.346\n",
      "Epoch: 325 Loss: 22.715 Accuracy: 0.350\n",
      "Epoch: 330 Loss: 22.520 Accuracy: 0.355\n",
      "Epoch: 335 Loss: 22.560 Accuracy: 0.359\n",
      "Epoch: 340 Loss: 22.669 Accuracy: 0.364\n",
      "Epoch: 345 Loss: 22.459 Accuracy: 0.368\n",
      "Epoch: 350 Loss: 22.339 Accuracy: 0.372\n",
      "Epoch: 355 Loss: 22.304 Accuracy: 0.376\n",
      "Epoch: 360 Loss: 22.327 Accuracy: 0.380\n",
      "Epoch: 365 Loss: 22.431 Accuracy: 0.384\n",
      "Epoch: 370 Loss: 22.404 Accuracy: 0.388\n",
      "Epoch: 375 Loss: 22.326 Accuracy: 0.392\n",
      "Epoch: 380 Loss: 22.195 Accuracy: 0.396\n",
      "Epoch: 385 Loss: 22.158 Accuracy: 0.400\n",
      "Epoch: 390 Loss: 22.152 Accuracy: 0.404\n",
      "Epoch: 395 Loss: 22.328 Accuracy: 0.408\n",
      "Epoch: 400 Loss: 21.989 Accuracy: 0.412\n",
      "Epoch: 405 Loss: 21.912 Accuracy: 0.415\n",
      "Epoch: 410 Loss: 22.064 Accuracy: 0.419\n",
      "Epoch: 415 Loss: 21.968 Accuracy: 0.423\n",
      "Epoch: 420 Loss: 21.861 Accuracy: 0.426\n",
      "Epoch: 425 Loss: 21.986 Accuracy: 0.429\n",
      "Epoch: 430 Loss: 21.952 Accuracy: 0.432\n",
      "Epoch: 435 Loss: 21.735 Accuracy: 0.436\n",
      "Epoch: 440 Loss: 21.684 Accuracy: 0.439\n",
      "Epoch: 445 Loss: 21.697 Accuracy: 0.442\n",
      "Epoch: 450 Loss: 21.864 Accuracy: 0.445\n",
      "Epoch: 455 Loss: 21.493 Accuracy: 0.449\n",
      "Epoch: 460 Loss: 21.469 Accuracy: 0.452\n",
      "Epoch: 465 Loss: 21.705 Accuracy: 0.455\n",
      "Epoch: 470 Loss: 21.376 Accuracy: 0.458\n",
      "Epoch: 475 Loss: 21.429 Accuracy: 0.461\n",
      "Epoch: 480 Loss: 21.733 Accuracy: 0.464\n",
      "Epoch: 485 Loss: 21.431 Accuracy: 0.467\n",
      "Epoch: 490 Loss: 21.418 Accuracy: 0.470\n",
      "Epoch: 495 Loss: 21.380 Accuracy: 0.473\n",
      "Epoch: 500 Loss: 21.250 Accuracy: 0.475\n",
      "Epoch: 505 Loss: 21.396 Accuracy: 0.478\n",
      "Epoch: 510 Loss: 21.363 Accuracy: 0.481\n",
      "Epoch: 515 Loss: 21.608 Accuracy: 0.484\n",
      "Epoch: 520 Loss: 21.471 Accuracy: 0.487\n",
      "Epoch: 525 Loss: 21.154 Accuracy: 0.489\n",
      "Epoch: 530 Loss: 21.162 Accuracy: 0.492\n",
      "Epoch: 535 Loss: 21.289 Accuracy: 0.495\n",
      "Epoch: 540 Loss: 21.258 Accuracy: 0.498\n",
      "Epoch: 545 Loss: 21.097 Accuracy: 0.500\n",
      "Epoch: 550 Loss: 21.076 Accuracy: 0.503\n",
      "Epoch: 555 Loss: 21.060 Accuracy: 0.505\n",
      "Epoch: 560 Loss: 20.843 Accuracy: 0.508\n",
      "Epoch: 565 Loss: 20.925 Accuracy: 0.510\n",
      "Epoch: 570 Loss: 21.001 Accuracy: 0.513\n",
      "Epoch: 575 Loss: 20.802 Accuracy: 0.515\n",
      "Epoch: 580 Loss: 20.804 Accuracy: 0.517\n",
      "Epoch: 585 Loss: 20.905 Accuracy: 0.520\n",
      "Epoch: 590 Loss: 20.962 Accuracy: 0.522\n",
      "Epoch: 595 Loss: 20.983 Accuracy: 0.524\n",
      "Epoch: 600 Loss: 20.839 Accuracy: 0.527\n",
      "Epoch: 605 Loss: 20.996 Accuracy: 0.529\n",
      "Epoch: 610 Loss: 20.784 Accuracy: 0.531\n",
      "Epoch: 615 Loss: 20.732 Accuracy: 0.533\n",
      "Epoch: 620 Loss: 20.728 Accuracy: 0.535\n",
      "Epoch: 625 Loss: 20.631 Accuracy: 0.537\n",
      "Epoch: 630 Loss: 20.644 Accuracy: 0.539\n",
      "Epoch: 635 Loss: 20.632 Accuracy: 0.542\n",
      "Epoch: 640 Loss: 20.415 Accuracy: 0.544\n",
      "Epoch: 645 Loss: 20.552 Accuracy: 0.546\n",
      "Epoch: 650 Loss: 20.476 Accuracy: 0.548\n",
      "Epoch: 655 Loss: 20.451 Accuracy: 0.550\n",
      "Epoch: 660 Loss: 20.686 Accuracy: 0.552\n",
      "Epoch: 665 Loss: 20.486 Accuracy: 0.554\n",
      "Epoch: 670 Loss: 20.582 Accuracy: 0.556\n",
      "Epoch: 675 Loss: 20.578 Accuracy: 0.558\n",
      "Epoch: 680 Loss: 20.407 Accuracy: 0.560\n",
      "Epoch: 685 Loss: 20.434 Accuracy: 0.562\n",
      "Epoch: 690 Loss: 20.237 Accuracy: 0.563\n",
      "Epoch: 695 Loss: 20.177 Accuracy: 0.565\n",
      "Epoch: 700 Loss: 20.384 Accuracy: 0.567\n",
      "Epoch: 705 Loss: 20.464 Accuracy: 0.569\n",
      "Epoch: 710 Loss: 20.070 Accuracy: 0.570\n",
      "Epoch: 715 Loss: 20.333 Accuracy: 0.572\n",
      "Epoch: 720 Loss: 20.067 Accuracy: 0.574\n",
      "Epoch: 725 Loss: 20.046 Accuracy: 0.576\n",
      "Epoch: 730 Loss: 20.056 Accuracy: 0.578\n",
      "Epoch: 735 Loss: 20.086 Accuracy: 0.579\n",
      "Epoch: 740 Loss: 20.022 Accuracy: 0.581\n",
      "Epoch: 745 Loss: 20.133 Accuracy: 0.583\n",
      "Epoch: 750 Loss: 19.963 Accuracy: 0.585\n",
      "Epoch: 755 Loss: 19.907 Accuracy: 0.586\n",
      "Epoch: 760 Loss: 19.938 Accuracy: 0.587\n",
      "Epoch: 765 Loss: 19.946 Accuracy: 0.589\n",
      "Epoch: 770 Loss: 20.054 Accuracy: 0.591\n",
      "Epoch: 775 Loss: 20.082 Accuracy: 0.593\n",
      "Epoch: 780 Loss: 19.772 Accuracy: 0.594\n",
      "Epoch: 785 Loss: 20.097 Accuracy: 0.596\n",
      "Epoch: 790 Loss: 19.725 Accuracy: 0.597\n",
      "Epoch: 795 Loss: 19.679 Accuracy: 0.599\n",
      "Epoch: 800 Loss: 19.851 Accuracy: 0.600\n",
      "Epoch: 805 Loss: 19.729 Accuracy: 0.602\n",
      "Epoch: 810 Loss: 19.687 Accuracy: 0.603\n",
      "Epoch: 815 Loss: 19.599 Accuracy: 0.605\n",
      "Epoch: 820 Loss: 19.817 Accuracy: 0.606\n",
      "Epoch: 825 Loss: 19.593 Accuracy: 0.608\n",
      "Epoch: 830 Loss: 19.790 Accuracy: 0.610\n",
      "Epoch: 835 Loss: 19.630 Accuracy: 0.611\n",
      "Epoch: 840 Loss: 19.543 Accuracy: 0.612\n",
      "Epoch: 845 Loss: 19.568 Accuracy: 0.614\n",
      "Epoch: 850 Loss: 19.545 Accuracy: 0.615\n",
      "Epoch: 855 Loss: 19.719 Accuracy: 0.616\n",
      "Epoch: 860 Loss: 19.434 Accuracy: 0.618\n",
      "Epoch: 865 Loss: 19.349 Accuracy: 0.619\n",
      "Epoch: 870 Loss: 19.339 Accuracy: 0.620\n",
      "Epoch: 875 Loss: 19.386 Accuracy: 0.622\n",
      "Epoch: 880 Loss: 19.312 Accuracy: 0.623\n",
      "Epoch: 885 Loss: 19.389 Accuracy: 0.624\n",
      "Epoch: 890 Loss: 19.571 Accuracy: 0.626\n",
      "Epoch: 895 Loss: 19.411 Accuracy: 0.627\n",
      "Epoch: 900 Loss: 19.350 Accuracy: 0.628\n",
      "Epoch: 905 Loss: 19.303 Accuracy: 0.630\n",
      "Epoch: 910 Loss: 19.346 Accuracy: 0.631\n",
      "Epoch: 915 Loss: 19.453 Accuracy: 0.632\n",
      "Epoch: 920 Loss: 19.243 Accuracy: 0.633\n",
      "Epoch: 925 Loss: 19.299 Accuracy: 0.634\n",
      "Epoch: 930 Loss: 19.085 Accuracy: 0.635\n",
      "Epoch: 935 Loss: 19.133 Accuracy: 0.637\n",
      "Epoch: 940 Loss: 19.087 Accuracy: 0.638\n",
      "Epoch: 945 Loss: 19.117 Accuracy: 0.639\n",
      "Epoch: 950 Loss: 19.285 Accuracy: 0.640\n",
      "Epoch: 955 Loss: 19.029 Accuracy: 0.641\n",
      "Epoch: 960 Loss: 19.043 Accuracy: 0.642\n",
      "Epoch: 965 Loss: 19.015 Accuracy: 0.644\n",
      "Epoch: 970 Loss: 18.922 Accuracy: 0.645\n",
      "Epoch: 975 Loss: 19.011 Accuracy: 0.646\n",
      "Epoch: 980 Loss: 18.908 Accuracy: 0.647\n",
      "Epoch: 985 Loss: 19.044 Accuracy: 0.648\n",
      "Epoch: 990 Loss: 18.745 Accuracy: 0.649\n",
      "Epoch: 995 Loss: 19.026 Accuracy: 0.650\n",
      "Overall Accuracy in predicting the test data =  percent 87.8\n",
      "saved Sample07866_pred\n",
      "saved Sample07503_pred\n",
      "saved Sample04447_pred\n",
      "saved Sample09687_pred\n",
      "saved Sample06572_pred\n",
      "saved Sample00847_pred\n",
      "saved Sample04150_pred\n",
      "saved Sample00253_pred\n",
      "saved Sample08746_pred\n",
      "saved Sample09208_pred\n",
      "saved step00999_weights.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def build_model(images):\n",
    "    '''\n",
    "    Defining a LeNet model for traffic sign classification\n",
    "    :param images:\n",
    "    :return: defined model\n",
    "    '''\n",
    "    with tf.name_scope(\"BNN\", values=[images]):\n",
    "        model = tf.keras.Sequential([\n",
    "            tfp.layers.Convolution2DFlipout(10,\n",
    "                                            kernel_size=5,\n",
    "                                            padding=\"VALID\",\n",
    "                                            activation=tf.nn.relu),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=[3, 3],\n",
    "                                         strides=[1, 1],\n",
    "                                         padding=\"VALID\"),\n",
    "            tfp.layers.Convolution2DFlipout(15,\n",
    "                                            kernel_size=3,\n",
    "                                            padding=\"VALID\",\n",
    "                                            activation=tf.nn.relu),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n",
    "                                         strides=[2, 2],\n",
    "                                         padding=\"VALID\"),\n",
    "            tfp.layers.Convolution2DFlipout(30,\n",
    "                                            kernel_size=3,\n",
    "                                            padding=\"VALID\",\n",
    "                                            activation=tf.nn.relu),\n",
    "\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n",
    "                                         strides=[2, 2],\n",
    "                                         padding=\"VALID\"),\n",
    "\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tfp.layers.DenseFlipout(400, activation=tf.nn.relu),\n",
    "            tfp.layers.DenseFlipout(120, activation = tf.nn.relu),\n",
    "            tfp.layers.DenseFlipout(84, activation=tf.nn.relu),\n",
    "            tfp.layers.DenseFlipout(43) ])\n",
    "\n",
    "        logits = model(images)\n",
    "        targets_distribution = tfd.Categorical(logits=logits)\n",
    "\n",
    "    return model,logits, targets_distribution\n",
    "\n",
    "\n",
    "def main():\n",
    "    #del argv #unused\n",
    "    print (\"Extracting dataset from zip files\")\n",
    "    extract_dataset()\n",
    "    X_train, y_train, X_test,y_test= load_preprocessed_data()\n",
    "    X_train_gray = load_grayscale_images(X_train)\n",
    "    X_test_gray = load_grayscale_images(X_test,data_type ='test')\n",
    "\n",
    "    # Shape of the dataset\n",
    "    print(\"Shape of X_train is \", X_train.shape)\n",
    "    print(\"Shape of X_test is \", X_test.shape)\n",
    "    print(\"Shape of y_train is \", y_train.shape)\n",
    "    print(\"Shape of y_test is \", y_test.shape)\n",
    "\n",
    "    print(\"Shape of X_train Grayscale is \", X_train_gray.shape)\n",
    "    print(\"Shape of X_test is Grayscale\", X_test_gray.shape)\n",
    "    print(\"Preprocessing Done\")\n",
    "\n",
    "    # Plotting the input data\n",
    "    #plot_input_data(X_train,y_train)\n",
    "\n",
    "    # Data Pipeline for modeling\n",
    "    (images, targets, iter_handle,\n",
    "     train_iterator, test_iterator) = build_data_pipeline(X_train_gray, X_test_gray,y_train, y_test)\n",
    "\n",
    "    #Building Model\n",
    "    model, logits, targets_distribution =build_model(images)\n",
    "\n",
    "    # Compute the -ELBO as the loss, averaged over the batch size.\n",
    "    neg_log_likelihood = -tf.reduce_mean(targets_distribution.log_prob(targets))\n",
    "    kl = sum(model.losses) / X_train.shape[0]\n",
    "    elbo_loss = neg_log_likelihood + kl\n",
    "\n",
    "    # Defining metrics for evalution\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    accuracy, accuracy_update_op = tf.metrics.accuracy(\n",
    "        labels=targets, predictions=predictions)\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "        train_op = optimizer.minimize(elbo_loss)\n",
    "\n",
    "    # Extract weight posterior statistics for layers with weight distributions\n",
    "    # for later visualization.\n",
    "    names = []\n",
    "    qmeans = []\n",
    "    qstds = []\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        try:\n",
    "            q = layer.kernel_posterior\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        names.append(\"Layer {}\".format(i))\n",
    "        qmeans.append(q.mean())\n",
    "        qstds.append(q.stddev())\n",
    "\n",
    "    # Initialize the variables\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "\n",
    "        # Run the training loop.\n",
    "        train_handle = sess.run(train_iterator.string_handle())\n",
    "        test_handle = sess.run(test_iterator.string_handle())\n",
    "        for step in range(EPOCHS):\n",
    "            _ = sess.run([train_op, accuracy_update_op],\n",
    "                         feed_dict={iter_handle: train_handle})\n",
    "\n",
    "            if step % 5== 0:\n",
    "                loss_value, accuracy_value = sess.run(\n",
    "                    [elbo_loss, accuracy], feed_dict={iter_handle: train_handle})\n",
    "                print(\"Epoch: {:>3d} Loss: {:.3f} Accuracy: {:.3f}\".format(\n",
    "                    step, loss_value, accuracy_value))\n",
    "\n",
    "        #Sampling from the posterior and obtaining mean probability for held out dataset\n",
    "        probs = np.asarray([sess.run((targets_distribution.probs),\n",
    "                                     feed_dict={iter_handle: test_handle})\n",
    "                            for _ in range(NUM_MONTE_CARLO)])\n",
    "        mean_probs = np.mean(probs, axis=0)\n",
    "\n",
    "        test_acc_dist = []\n",
    "        for prob in probs:\n",
    "            y_test_pred = np.argmax(prob, axis=1).astype(np.float32)\n",
    "            accuracy = (y_test_pred == y_test).mean() * 100\n",
    "            test_acc_dist.append(accuracy)\n",
    "\n",
    "        plt.hist(test_acc_dist)\n",
    "        plt.title(\"Histogram of prediction accuracies on test dataset\")\n",
    "        plt.xlabel(\"Accuracy\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        save_dir = os.path.join(DATA_DIR, \"..\", \"plots\")\n",
    "        plt.savefig(os.path.join(save_dir,  \"Test_Dataset_Prediction_Accuracy.png\"))\n",
    "\n",
    "        # Get the average accuracy\n",
    "        Y_pred = np.argmax(mean_probs, axis=1)\n",
    "        print(\"Overall Accuracy in predicting the test data =  percent\", round((Y_pred == y_test).mean() * 100,2))\n",
    "        # Draw two random samples from the test data\n",
    "        sample_images_idx= np.random.choice(range(X_test_gray.shape[0]), size=10)\n",
    "        for i in sample_images_idx:\n",
    "            sampled_image = X_test_gray[i]\n",
    "            sample_label = y_test[i]\n",
    "            mean_prediction = Y_pred[i]\n",
    "            plot_heldout_prediction(sampled_image, probs[:,i,:],\n",
    "                                    fname=\"Sample{:05d}_pred\".format(i),\n",
    "                                    title=\"Correct Label {:02d}, Mean Prediction {:02d}\"\n",
    "                                    .format(sample_label,mean_prediction))\n",
    "\n",
    "        qm_vals, qs_vals = sess.run((qmeans, qstds))\n",
    "\n",
    "        # Plotting Weight Means and Standard deviation\n",
    "        plot_weight_posteriors(names, qm_vals, qs_vals,\n",
    "                               fname=\"step{:05d}_weights.png\".format(step))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
