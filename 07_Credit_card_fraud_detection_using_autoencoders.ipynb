{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data from here: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This file defines the parameters for the project\n",
    "'''\n",
    "\n",
    "## Directories\n",
    "\n",
    "DATA_DIR = \"./datasetslib/data\"\n",
    "PLOTS_DIR = \"./datasetslib/plots\"\n",
    "MODEL_SAVE_DIR = \"./datasetslib/saved_models\"\n",
    "LOG_DIR = \"./datasetslib/logs\"\n",
    "\n",
    "\n",
    "## Training Parameters\n",
    "RANDOM_SEED = 0\n",
    "DIM_ENCODER = 14\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS = 'mean_squared_error'\n",
    "EVAL_METRIC = 'accuracy'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    zf = zipfile.ZipFile(os.path.join(DATA_DIR,\"creditcardfraud.zip\"))\n",
    "    data = pd.read_csv(zf.open(\"creditcard.csv\"))\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data = data.drop(['Time'], axis=1)\n",
    "    data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    data = read_data()\n",
    "    processed_data = preprocess_data(data)\n",
    "    return processed_data\n",
    "\n",
    "def get_train_and_test_data(processed_data):\n",
    "    X_train, X_test = train_test_split(processed_data, test_size=0.25, random_state=RANDOM_SEED)\n",
    "    X_train = X_train[X_train.Class == 0]\n",
    "    X_train = X_train.drop(['Class'], axis=1)\n",
    "\n",
    "    y_test = X_test['Class']\n",
    "    X_test = X_test.drop(['Class'], axis=1)\n",
    "\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    return X_train, X_test,y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This file contains the function to build an autoencoder from Tensorflow/Keras\n",
    "'''\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "\n",
    "\n",
    "class MODEL():\n",
    "    def __init__(self,train_data,test_data,y_test):\n",
    "        # Defining Data Variables\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.y_test = y_test\n",
    "\n",
    "        #Defining the model\n",
    "        self.model = self.define_model()\n",
    "\n",
    "        # Create Directories\n",
    "        if not os.path.exists(MODEL_SAVE_DIR):\n",
    "            os.makedirs(MODEL_SAVE_DIR)\n",
    "\n",
    "        if not os.path.exists(LOG_DIR):\n",
    "            os.makedirs(LOG_DIR)\n",
    "\n",
    "        if not os.path.exists(PLOTS_DIR):\n",
    "            os.makedirs(PLOTS_DIR)\n",
    "\n",
    "\n",
    "    def define_model(self):\n",
    "        dim_input = self.train_data.shape[1]\n",
    "        layer_input = Input(shape=(dim_input,))\n",
    "\n",
    "        layer_encoder = Dense(DIM_ENCODER, activation=\"tanh\",\n",
    "                              activity_regularizer=regularizers.l1(10e-5))(layer_input)\n",
    "        layer_encoder = Dense(int(DIM_ENCODER / 2), activation=\"relu\")(layer_encoder)\n",
    "\n",
    "        layer_decoder = Dense(int(DIM_ENCODER / 2), activation='tanh')(layer_encoder)\n",
    "        layer_decoder = Dense(dim_input, activation='relu')(layer_decoder)\n",
    "\n",
    "        autoencoder = Model(inputs=layer_input, outputs=layer_decoder)\n",
    "        return autoencoder\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        self.model.compile(optimizer=OPTIMIZER,\n",
    "                      loss=LOSS,\n",
    "                      metrics=[EVAL_METRIC])\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath=os.path.join(MODEL_SAVE_DIR, \"trained_model.h5\"),\n",
    "                                     verbose=0,\n",
    "                                     save_best_only=True)\n",
    "        log_tensorboard = TensorBoard(log_dir='./logs',\n",
    "                                      histogram_freq=0,\n",
    "                                      write_graph=True,\n",
    "                                      write_images=True)\n",
    "\n",
    "        history = self.model.fit(self.train_data, self.train_data,\n",
    "                             epochs=EPOCHS,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                             validation_data=(self.test_data, self.test_data),\n",
    "                             verbose=1,\n",
    "                             callbacks=[checkpoint, log_tensorboard]).history\n",
    "        self.history = history\n",
    "        print(\"Training Done. Plotting Loss Curves\")\n",
    "        self.plot_loss_curves()\n",
    "\n",
    "    def plot_loss_curves(self):\n",
    "        fig = plt.figure(num=\"Loss Curves\")\n",
    "        fig.set_size_inches(12, 6)\n",
    "        plt.plot(self.history['loss'])\n",
    "        plt.plot(self.history['val_loss'])\n",
    "        plt.title('Loss By Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch Num')\n",
    "        plt.legend(['Train_Data', 'Test_Data'], loc='upper right');\n",
    "        plt.grid(True, alpha=.25)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        image_name = 'Loss_Curves.png'\n",
    "        fig.savefig(os.path.join(PLOTS_DIR,image_name), dpi=fig.dpi)\n",
    "        plt.clf()\n",
    "\n",
    "    def get_trained_model(self):\n",
    "        self.model = load_model(os.path.join(MODEL_SAVE_DIR, \"trained_model.h5\"))\n",
    "\n",
    "    def get_test_predictions(self):\n",
    "        self.test_predictions = self.model.predict(self.test_data)\n",
    "\n",
    "    def plot_reconstruction_error_by_class(self):\n",
    "        self.get_test_predictions()\n",
    "        mse = np.mean(np.power(self.test_data - self.test_predictions, 2), axis=1)\n",
    "        self.recon_error = pd.DataFrame({'recon_error': mse,\n",
    "                                 'true_class': self.y_test})\n",
    "\n",
    "        ## Plotting the errors by class\n",
    "        # Normal Transactions\n",
    "        fig = plt.figure(num = \"Recon Error with Normal Transactions\")\n",
    "        fig.set_size_inches(12, 6)\n",
    "        ax = fig.add_subplot(111)\n",
    "        normal_error_df = self.recon_error[(self.recon_error['true_class'] == 0) & (self.recon_error['recon_error'] < 50)]\n",
    "        _ = ax.hist(normal_error_df.recon_error.values, bins=20)\n",
    "        plt.xlabel(\"Recon Error Bins\")\n",
    "        plt.ylabel(\"Num Samples\")\n",
    "        plt.title(\"Recon Error with Normal Transactions\")\n",
    "        plt.tight_layout()\n",
    "        image_name = \"Recon_Error_with_Normal_Transactions.png\"\n",
    "        fig.savefig(os.path.join(PLOTS_DIR, image_name), dpi=fig.dpi)\n",
    "        plt.clf()\n",
    "\n",
    "        # Fraud Transactions\n",
    "        fig = plt.figure(num=\"Recon Error with Fraud Transactions\")\n",
    "        fig.set_size_inches(12, 6)\n",
    "        ax = fig.add_subplot(111)\n",
    "        fraud_error_df = self.recon_error[(self.recon_error['true_class'] == 1)]\n",
    "        _ = ax.hist(fraud_error_df.recon_error.values, bins=20)\n",
    "        plt.xlabel(\"Recon Error Bins\")\n",
    "        plt.ylabel(\"Num Samples\")\n",
    "        plt.title(\"Recon Error with Fraud Transactions\")\n",
    "        plt.tight_layout()\n",
    "        image_name = \"Recon_Error_with_Fraud_Transactions.png\"\n",
    "        fig.savefig(os.path.join(PLOTS_DIR, image_name), dpi=fig.dpi)\n",
    "        plt.clf()\n",
    "\n",
    "    def get_precision_recall_curves(self):\n",
    "        precision, recall, threshold = precision_recall_curve(self.recon_error.true_class, self.recon_error.recon_error)\n",
    "        # Plotting the precision curve\n",
    "        fig = plt.figure(num =\"Precision Curve\")\n",
    "        fig.set_size_inches(12, 6)\n",
    "\n",
    "        plt.plot(threshold, precision[1:], 'g', label='Precision curve')\n",
    "        plt.title('Precision By Recon Error Threshold Values')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlim(0,200)\n",
    "        plt.tight_layout()\n",
    "        image_name = 'Precision_Threshold_Curve.png'\n",
    "        fig.savefig(os.path.join(PLOTS_DIR, image_name), dpi=fig.dpi)\n",
    "        plt.clf()\n",
    "\n",
    "        plt.plot(threshold, recall[1:], 'g', label='Recall curve')\n",
    "        plt.title('Recall By Recon Error Threshold Values')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.tight_layout()\n",
    "        image_name = 'Recall_Threshold_Curve.png'\n",
    "        fig.savefig(os.path.join(PLOTS_DIR, image_name), dpi=fig.dpi)\n",
    "        plt.clf()\n",
    "\n",
    "    def get_confusion_matrix(self, min_recall = 0.8):\n",
    "        # Get the confusion matrix with min desired recall on the testing dataset used.\n",
    "        precision, recall, threshold = precision_recall_curve(self.recon_error.true_class, self.recon_error.recon_error)\n",
    "        idx = list(filter(lambda x: x[1] > min_recall, enumerate(recall[1:])))[-1][0]\n",
    "        th = threshold[idx]\n",
    "        print (\"Min recall is : %f, Threshold for recon error is: %f \" %(recall[idx+1], th))\n",
    "\n",
    "        # Get the confusion matrix\n",
    "        predicted_class = [1 if e > th else 0 for e in self.recon_error.recon_error.values]\n",
    "        cnf_matrix = confusion_matrix(self.recon_error.true_class, predicted_class)\n",
    "        classes = ['Normal','Fraud']\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 12))\n",
    "        plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        fmt = 'd'\n",
    "        thresh = cnf_matrix.max() / 2.\n",
    "        for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "            plt.text(j, i, format(cnf_matrix[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        image_name = 'Confusion_Matrix_with_threshold_{}.png'.format(th)\n",
    "        fig.savefig(os.path.join(PLOTS_DIR, image_name), dpi=fig.dpi)\n",
    "        plt.clf()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data\n",
      "Getting train and test dataset\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Training the model\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 213233 samples, validate on 71202 samples\n",
      "Epoch 1/10\n",
      "213233/213233 [==============================] - 15s 71us/step - loss: 0.8169 - acc: 0.5988 - val_loss: 0.8048 - val_acc: 0.6716\n",
      "Epoch 2/10\n",
      "213233/213233 [==============================] - 15s 70us/step - loss: 0.7421 - acc: 0.6759 - val_loss: 0.7798 - val_acc: 0.6782\n",
      "Epoch 3/10\n",
      "213233/213233 [==============================] - 15s 70us/step - loss: 0.7282 - acc: 0.6814 - val_loss: 0.7724 - val_acc: 0.6819\n",
      "Epoch 4/10\n",
      "213233/213233 [==============================] - 15s 70us/step - loss: 0.7228 - acc: 0.6880 - val_loss: 0.7687 - val_acc: 0.6883\n",
      "Epoch 5/10\n",
      "213233/213233 [==============================] - 15s 70us/step - loss: 0.7201 - acc: 0.6906 - val_loss: 0.7658 - val_acc: 0.6897\n",
      "Epoch 6/10\n",
      "213233/213233 [==============================] - 15s 70us/step - loss: 0.7181 - acc: 0.6918 - val_loss: 0.7646 - val_acc: 0.6830\n",
      "Epoch 7/10\n",
      "213233/213233 [==============================] - 15s 70us/step - loss: 0.7169 - acc: 0.6939 - val_loss: 0.7682 - val_acc: 0.6846\n",
      "Epoch 8/10\n",
      "213233/213233 [==============================] - 15s 70us/step - loss: 0.7160 - acc: 0.6937 - val_loss: 0.7639 - val_acc: 0.6877\n",
      "Epoch 9/10\n",
      "213233/213233 [==============================] - 15s 71us/step - loss: 0.7152 - acc: 0.6940 - val_loss: 0.7616 - val_acc: 0.6962\n",
      "Epoch 10/10\n",
      "213233/213233 [==============================] - 15s 70us/step - loss: 0.7145 - acc: 0.6946 - val_loss: 0.7667 - val_acc: 0.6750\n",
      "Training Done. Plotting Loss Curves\n",
      "Loading the trained model\n",
      "Get Reconstruction Loss By Class\n",
      "Getting Precision Recall Curves by Thresholds\n",
      "Get confusion matrix with 80% recall on Test Dataset\n",
      "Min recall is : 0.808333, Threshold for recon error is: 3.811357 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    # Loading and processing data\n",
    "    print (\"Loading the data\")\n",
    "    processed_data = load_and_preprocess_data()\n",
    "    print (\"Getting train and test dataset\")\n",
    "    X_train, X_test, y_test = get_train_and_test_data(processed_data)\n",
    "\n",
    "    model_obj = MODEL(X_train,X_test,y_test)\n",
    "    print (\"Training the model\")\n",
    "    model_obj.train_model()\n",
    "    print (\"Loading the trained model\")\n",
    "    model_obj.get_trained_model()\n",
    "    print (\"Get Reconstruction Loss By Class\")\n",
    "    model_obj.plot_reconstruction_error_by_class()\n",
    "    print (\"Getting Precision Recall Curves by Thresholds\")\n",
    "    model_obj.get_precision_recall_curves()\n",
    "    print (\"Get confusion matrix with 80% recall on Test Dataset\")\n",
    "    model_obj.get_confusion_matrix(min_recall = 0.8)\n",
    "\n",
    "\n",
    "if __name__==main():\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
