{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data from here: http://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0002\n",
    "EPOCHS = 1111\n",
    "RESTORE_TRAINING= False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This file consists of the helper functions for processing\n",
    "'''\n",
    "\n",
    "## Package Imports\n",
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "try:\n",
    "    import wget\n",
    "except:\n",
    "    print (\"Can't import wget as you are probably on windows laptop\")\n",
    "\n",
    "def extract_files(data_dir,type = 'bags'):\n",
    "    '''\n",
    "    :param data_dir: Input directory\n",
    "    :param type: bags or shoes\n",
    "    :return: saves the cropped files to the bags to shoes directory\n",
    "    '''\n",
    "    input_file_dir = os.path.join(os.getcwd(),data_dir, \"train\")\n",
    "    result_dir = os.path.join(os.getcwd(),type)\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "\n",
    "    file_names= os.listdir(input_file_dir)\n",
    "    for file in file_names:\n",
    "        input_image = Image.open(os.path.join(input_file_dir,file))\n",
    "        input_image = input_image.resize([128, 64])\n",
    "        input_image = input_image.crop([64, 0, 128, 64])  # Cropping only the colored image. Excluding the edge image\n",
    "        input_image.save(os.path.join(result_dir,file))\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    '''\n",
    "    Before executing this function. Follow these steps;\n",
    "    1. Download the datasets\n",
    "    Handbags data Link 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz'\n",
    "    Shoes data Link 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz'\n",
    "\n",
    "    2. Extract the tar files.\n",
    "\n",
    "    3. Execute this function. This function will extract the handbags and shoe images from the datasets.\n",
    "    '''\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), \"edges2handbags\")):\n",
    "        try:\n",
    "            print (\"Downloading dataset\")\n",
    "            bag_data_link = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz'\n",
    "            shoe_data_link = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz'\n",
    "    \n",
    "            wget.download(bag_data_link)\n",
    "            wget.download(shoe_data_link)\n",
    "    \n",
    "            with tarfile.open('./edges2handbags.tar.gz') as tar:\n",
    "                tar.extractall()\n",
    "                tar.close()\n",
    "    \n",
    "            with tarfile.open('./edges2shoes.tar.gz') as tar:\n",
    "                tar.extractall()\n",
    "                tar.close()\n",
    "        except:\n",
    "            print (\"It seems you are on windows laptop. Please download the data as instructed in README before executing the code\")\n",
    "\n",
    "    extract_files(\"edges2handbags\", 'bags')\n",
    "    extract_files(\"edges2shoes\", 'shoes')\n",
    "\n",
    "\n",
    "def load_data(load_type = 'train'):\n",
    "    shoelist = glob.glob(os.path.join(os.getcwd(), \"shoes/*jpg\"))\n",
    "    shoe_data = np.array([np.array(Image.open(fname)) for fname in shoelist]).astype(np.float32)\n",
    "    baglist = glob.glob(os.path.join(os.getcwd(), \"bags/*jpg\"))\n",
    "    bags_data = np.array([np.array(Image.open(fname)) for fname in baglist]).astype(np.float32)\n",
    "    shoe_data = shoe_data/255.\n",
    "    bags_data = bags_data/255.\n",
    "    return shoe_data, bags_data\n",
    "\n",
    "\n",
    "def save_image(global_step, img_data, file_name):\n",
    "    sample_results_dir = os.path.join(os.getcwd(), \"sample_results\", \"epoch_\" +str(global_step))\n",
    "    if not os.path.exists(sample_results_dir):\n",
    "        os.makedirs(sample_results_dir)\n",
    "\n",
    "\n",
    "    result = Image.fromarray((img_data[0] * 255).astype(np.uint8))\n",
    "    result.save(os.path.join(sample_results_dir, file_name + \".jpg\"))\n",
    "\n",
    "\n",
    "\n",
    "def discriminator(x,initializer, scope_name ='discriminator',  reuse=False):\n",
    "    with tf.variable_scope(scope_name) as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        conv1 = tf.contrib.layers.conv2d(inputs=x, num_outputs=32, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, weights_initializer=initializer,\n",
    "                                         scope=\"disc_conv1\")  # 32 x 32 x 32\n",
    "        conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv2\")  # 16 x 16 x 64\n",
    "        conv3 = tf.contrib.layers.conv2d(inputs=conv2, num_outputs=128, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv3\")  # 8 x 8 x 128\n",
    "        conv4 = tf.contrib.layers.conv2d(inputs=conv3, num_outputs=256, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv4\")  # 4 x 4 x 256\n",
    "        conv5 = tf.contrib.layers.conv2d(inputs=conv4, num_outputs=512, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv5\")  # 2 x 2 x 512\n",
    "        fc1 = tf.reshape(conv5, shape=[tf.shape(x)[0], 2 * 2 * 512])\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=512, reuse=reuse, activation_fn=tf.nn.leaky_relu,\n",
    "                                                normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                                weights_initializer=initializer, scope=\"disc_fc1\")\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1, reuse=reuse, activation_fn=tf.nn.sigmoid,\n",
    "                                                weights_initializer=initializer, scope=\"disc_fc2\")\n",
    "\n",
    "        return fc2\n",
    "\n",
    "\n",
    "def generator(x, initializer, scope_name = 'generator',reuse=False):\n",
    "    with tf.variable_scope(scope_name) as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        conv1 = tf.contrib.layers.conv2d(inputs=x, num_outputs=32, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, weights_initializer=initializer,\n",
    "                                         scope=\"disc_conv1\")  # 32 x 32 x 32\n",
    "        conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv2\")  # 16 x 16 x 64\n",
    "        conv3 = tf.contrib.layers.conv2d(inputs=conv2, num_outputs=128, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv3\")  # 8 x 8 x 128\n",
    "        conv4 = tf.contrib.layers.conv2d(inputs=conv3, num_outputs=256, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv4\")  # 4 x 4 x 256\n",
    "\n",
    "        deconv1 = tf.contrib.layers.conv2d(conv4, num_outputs=4 * 128, kernel_size=4, stride=1, padding=\"SAME\",\n",
    "                                               activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                               weights_initializer=initializer, scope=\"gen_conv1\")\n",
    "        deconv1 = tf.reshape(deconv1, shape=[tf.shape(x)[0], 8, 8, 128])\n",
    "\n",
    "        deconv2 = tf.contrib.layers.conv2d(deconv1, num_outputs=4 * 64, kernel_size=4, stride=1, padding=\"SAME\",\n",
    "                                               activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                               weights_initializer=initializer, scope=\"gen_conv2\")\n",
    "        deconv2 = tf.reshape(deconv2, shape=[tf.shape(x)[0], 16, 16, 64])\n",
    "\n",
    "        deconv3 = tf.contrib.layers.conv2d(deconv2, num_outputs=4 * 32, kernel_size=4, stride=1, padding=\"SAME\",\n",
    "                                               activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                               weights_initializer=initializer, scope=\"gen_conv3\")\n",
    "        deconv3 = tf.reshape(deconv3, shape=[tf.shape(x)[0], 32, 32, 32])\n",
    "\n",
    "        deconv4 = tf.contrib.layers.conv2d(deconv3, num_outputs=4 * 16, kernel_size=4, stride=1, padding=\"SAME\",\n",
    "                                               activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                               weights_initializer=initializer, scope=\"gen_conv4\")\n",
    "        deconv4 = tf.reshape(deconv4, shape=[tf.shape(x)[0], 64, 64, 16])\n",
    "\n",
    "        recon = tf.contrib.layers.conv2d(deconv4, num_outputs=3, kernel_size=4, stride=1, padding=\"SAME\", \\\n",
    "                                             activation_fn=tf.nn.relu, scope=\"gen_conv5\")\n",
    "\n",
    "        return recon\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a DiscoGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "class DiscoGAN:\n",
    "    def __init__(self):\n",
    "        with tf.variable_scope('Input'):\n",
    "            self.X_bags = tf.placeholder(shape = [None, 64, 64, 3], name='bags', dtype=tf.float32)\n",
    "            self.X_shoes = tf.placeholder(shape= [None, 64, 64, 3], name='shoes',dtype= tf.float32)\n",
    "        self.initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "        self.define_network()\n",
    "        self.define_loss()\n",
    "        self.get_trainable_params()\n",
    "        self.define_optimizer()\n",
    "        self.summary_()\n",
    "\n",
    "    def define_network(self):\n",
    "        \n",
    "        # Generators\n",
    "        # This one is used to generate fake data\n",
    "        self.gen_b_fake = generator(self.X_shoes, self.initializer,scope_name=\"generator_sb\")\n",
    "        self.gen_s_fake =   generator(self.X_bags, self.initializer,scope_name=\"generator_bs\")\n",
    "\n",
    "        # Reconstruction Generators\n",
    "        # Note that parameters are being used from previous layers\n",
    "        self.gen_recon_s = generator(self.gen_b_fake, self.initializer,scope_name=\"generator_sb\",  reuse=True)\n",
    "        self.gen_recon_b = generator(self.gen_s_fake,  self.initializer, scope_name=\"generator_bs\", reuse=True)\n",
    "\n",
    "        # Discriminator for Shoes\n",
    "        self.disc_s_real = discriminator(self.X_shoes,self.initializer, scope_name=\"discriminator_s\")\n",
    "        self.disc_s_fake = discriminator(self.gen_s_fake,self.initializer, scope_name=\"discriminator_s\", reuse=True)\n",
    "\n",
    "        # Discriminator for Bags\n",
    "        self.disc_b_real = discriminator(self.X_bags,self.initializer,scope_name=\"discriminator_b\")\n",
    "        self.disc_b_fake = discriminator(self.gen_b_fake, self.initializer, reuse=True,scope_name=\"discriminator_b\")\n",
    "\n",
    "        # Defining Discriminators of Bags and Shoes\n",
    "\n",
    "    def define_loss(self):\n",
    "        # Reconstruction loss for generators\n",
    "        self.const_loss_s = tf.reduce_mean(tf.losses.mean_squared_error(self.gen_recon_s, self.X_shoes))\n",
    "        self.const_loss_b = tf.reduce_mean(tf.losses.mean_squared_error(self.gen_recon_b, self.X_bags))\n",
    "\n",
    "        # Generator loss for GANs\n",
    "        self.gen_s_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_fake, labels=tf.ones_like(self.disc_s_fake)))\n",
    "        self.gen_b_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_fake, labels=tf.ones_like(self.disc_b_fake)))\n",
    "\n",
    "        # Total Generator Loss\n",
    "        self.gen_loss =  (self.const_loss_b + self.const_loss_s)  + self.gen_s_loss + self.gen_b_loss\n",
    "\n",
    "        # Cross Entropy loss for discriminators for shoes and bags\n",
    "        # Shoes\n",
    "        self.disc_s_real_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_real, labels=tf.ones_like(self.disc_s_real)))\n",
    "        self.disc_s_fake_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_fake, labels=tf.zeros_like(self.disc_s_fake)))\n",
    "        self.disc_s_loss = self.disc_s_real_loss + self.disc_s_fake_loss  # Combined\n",
    "\n",
    "\n",
    "        # Bags\n",
    "        self.disc_b_real_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_real, labels=tf.ones_like(self.disc_b_real)))\n",
    "        self.disc_b_fake_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_fake, labels=tf.zeros_like(self.disc_b_fake)))\n",
    "        self.disc_b_loss = self.disc_b_real_loss + self.disc_b_fake_loss\n",
    "\n",
    "        # Total Discriminator Loss\n",
    "        self.disc_loss = self.disc_b_loss + self.disc_s_loss\n",
    "\n",
    "    def get_trainable_params(self):\n",
    "        '''\n",
    "        This function is useful for obtaining trainable parameters which need to be trained either with discriminator or generator loss\n",
    "        :return:\n",
    "        '''\n",
    "        self.disc_params = []\n",
    "        self.gen_params = []\n",
    "        for var in tf.trainable_variables():\n",
    "            if 'generator' in var.name:\n",
    "                self.gen_params.append(var)\n",
    "            elif 'discriminator' in var.name:\n",
    "                self.disc_params.append(var)\n",
    "\n",
    "    def define_optimizer(self):\n",
    "        self.disc_optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.disc_loss, var_list=self.disc_params)\n",
    "        self.gen_optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.gen_loss, var_list=self.gen_params)\n",
    "\n",
    "    def summary_(self):\n",
    "        # Store the losses\n",
    "        tf.summary.scalar(\"gen_loss\", self.gen_loss)\n",
    "        tf.summary.scalar(\"gen_s_loss\", self.gen_s_loss)\n",
    "        tf.summary.scalar(\"gen_b_loss\", self.gen_b_loss)\n",
    "        tf.summary.scalar(\"const_loss_s\", self.const_loss_s)\n",
    "        tf.summary.scalar(\"const_loss_b\", self.const_loss_b)\n",
    "        tf.summary.scalar(\"disc_loss\", self.disc_loss)\n",
    "        tf.summary.scalar(\"disc_b_loss\", self.disc_b_loss)\n",
    "        tf.summary.scalar(\"disc_s_loss\", self.disc_s_loss)\n",
    "\n",
    "        # Histograms for all vars\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.name, var)\n",
    "\n",
    "        self.summary_ = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the model\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv1/weights:0 is illegal; using generator_sb/disc_conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv1/biases:0 is illegal; using generator_sb/disc_conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv2/weights:0 is illegal; using generator_sb/disc_conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv2/BatchNorm/beta:0 is illegal; using generator_sb/disc_conv2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv3/weights:0 is illegal; using generator_sb/disc_conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv3/BatchNorm/beta:0 is illegal; using generator_sb/disc_conv3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv4/weights:0 is illegal; using generator_sb/disc_conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv4/BatchNorm/beta:0 is illegal; using generator_sb/disc_conv4/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv1/weights:0 is illegal; using generator_sb/gen_conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv1/BatchNorm/beta:0 is illegal; using generator_sb/gen_conv1/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv2/weights:0 is illegal; using generator_sb/gen_conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv2/BatchNorm/beta:0 is illegal; using generator_sb/gen_conv2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv3/weights:0 is illegal; using generator_sb/gen_conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv3/BatchNorm/beta:0 is illegal; using generator_sb/gen_conv3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv4/weights:0 is illegal; using generator_sb/gen_conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv4/BatchNorm/beta:0 is illegal; using generator_sb/gen_conv4/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv5/weights:0 is illegal; using generator_sb/gen_conv5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv5/biases:0 is illegal; using generator_sb/gen_conv5/biases_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv1/weights:0 is illegal; using generator_bs/disc_conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv1/biases:0 is illegal; using generator_bs/disc_conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv2/weights:0 is illegal; using generator_bs/disc_conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv2/BatchNorm/beta:0 is illegal; using generator_bs/disc_conv2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv3/weights:0 is illegal; using generator_bs/disc_conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv3/BatchNorm/beta:0 is illegal; using generator_bs/disc_conv3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv4/weights:0 is illegal; using generator_bs/disc_conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv4/BatchNorm/beta:0 is illegal; using generator_bs/disc_conv4/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv1/weights:0 is illegal; using generator_bs/gen_conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv1/BatchNorm/beta:0 is illegal; using generator_bs/gen_conv1/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv2/weights:0 is illegal; using generator_bs/gen_conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv2/BatchNorm/beta:0 is illegal; using generator_bs/gen_conv2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv3/weights:0 is illegal; using generator_bs/gen_conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv3/BatchNorm/beta:0 is illegal; using generator_bs/gen_conv3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv4/weights:0 is illegal; using generator_bs/gen_conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv4/BatchNorm/beta:0 is illegal; using generator_bs/gen_conv4/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv5/weights:0 is illegal; using generator_bs/gen_conv5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv5/biases:0 is illegal; using generator_bs/gen_conv5/biases_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv1/weights:0 is illegal; using discriminator_s/disc_conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv1/biases:0 is illegal; using discriminator_s/disc_conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv2/weights:0 is illegal; using discriminator_s/disc_conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv2/BatchNorm/beta:0 is illegal; using discriminator_s/disc_conv2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv3/weights:0 is illegal; using discriminator_s/disc_conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv3/BatchNorm/beta:0 is illegal; using discriminator_s/disc_conv3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv4/weights:0 is illegal; using discriminator_s/disc_conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv4/BatchNorm/beta:0 is illegal; using discriminator_s/disc_conv4/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv5/weights:0 is illegal; using discriminator_s/disc_conv5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv5/BatchNorm/beta:0 is illegal; using discriminator_s/disc_conv5/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_fc1/weights:0 is illegal; using discriminator_s/disc_fc1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_fc1/BatchNorm/beta:0 is illegal; using discriminator_s/disc_fc1/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_fc2/weights:0 is illegal; using discriminator_s/disc_fc2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_fc2/biases:0 is illegal; using discriminator_s/disc_fc2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv1/weights:0 is illegal; using discriminator_b/disc_conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv1/biases:0 is illegal; using discriminator_b/disc_conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv2/weights:0 is illegal; using discriminator_b/disc_conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv2/BatchNorm/beta:0 is illegal; using discriminator_b/disc_conv2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv3/weights:0 is illegal; using discriminator_b/disc_conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv3/BatchNorm/beta:0 is illegal; using discriminator_b/disc_conv3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv4/weights:0 is illegal; using discriminator_b/disc_conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv4/BatchNorm/beta:0 is illegal; using discriminator_b/disc_conv4/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv5/weights:0 is illegal; using discriminator_b/disc_conv5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv5/BatchNorm/beta:0 is illegal; using discriminator_b/disc_conv5/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_fc1/weights:0 is illegal; using discriminator_b/disc_fc1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_fc1/BatchNorm/beta:0 is illegal; using discriminator_b/disc_fc1/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_fc2/weights:0 is illegal; using discriminator_b/disc_fc2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_fc2/biases:0 is illegal; using discriminator_b/disc_fc2/biases_0 instead.\n",
      "Training\n",
      "Loading Dataset\n",
      "Starting Training\n",
      "EPOCH:0\tGenerator Loss: 1.9383547\tDiscriminator Loss: 2.8996198\n",
      "Saved Model\n",
      "EPOCH:100\tGenerator Loss: 1.2432686\tDiscriminator Loss: 2.8590252\n",
      "EPOCH:200\tGenerator Loss: 1.2120309\tDiscriminator Loss: 2.9504445\n",
      "EPOCH:300\tGenerator Loss: 1.4419038\tDiscriminator Loss: 2.6667628\n",
      "EPOCH:400\tGenerator Loss: 1.3874335\tDiscriminator Loss: 2.5181808\n",
      "EPOCH:500\tGenerator Loss: 1.4444473\tDiscriminator Loss: 2.530342\n",
      "EPOCH:600\tGenerator Loss: 1.4317666\tDiscriminator Loss: 2.5380833\n",
      "EPOCH:700\tGenerator Loss: 1.3994664\tDiscriminator Loss: 2.5358639\n",
      "EPOCH:800\tGenerator Loss: 1.3704668\tDiscriminator Loss: 2.552126\n",
      "EPOCH:900\tGenerator Loss: 1.4667308\tDiscriminator Loss: 2.461553\n",
      "EPOCH:1000\tGenerator Loss: 1.4584953\tDiscriminator Loss: 2.4599276\n",
      "Saved Model\n",
      "EPOCH:1100\tGenerator Loss: 1.4578745\tDiscriminator Loss: 2.4482665\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    # Load the data first\n",
    "    # Define a function to load the next batch\n",
    "    # start training\n",
    "\n",
    "    # Define a function to get the data for the next batch\n",
    "    def get_next_batch(BATCH_SIZE, type =\"shoes\"):\n",
    "        if type == \"shoes\":\n",
    "            next_batch_indices = random.sample(range(0, X_shoes.shape[0]), BATCH_SIZE)\n",
    "            batch_data = X_shoes[next_batch_indices,:,:,:]\n",
    "        elif type == \"bags\":\n",
    "            next_batch_indices = random.sample(range(0, X_bags.shape[0]), BATCH_SIZE)\n",
    "            batch_data = X_bags[next_batch_indices, :, :, :]\n",
    "        return batch_data\n",
    "\n",
    "    # Loading the dataset\n",
    "    print (\"Loading Dataset\")\n",
    "    X_shoes, X_bags = load_data(load_type='train')\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        if RESTORE_TRAINING:\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(\"./model\")\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('Model Loaded')\n",
    "            start_epoch = int(str(ckpt.model_checkpoint_path).split('-')[-1].split(\".\")[0])\n",
    "            print (\"Start EPOCH\", start_epoch)\n",
    "        else:\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            tf.global_variables_initializer().run()\n",
    "            if not os.path.exists(\"logs\"):\n",
    "                os.makedirs(\"logs\")\n",
    "            start_epoch = 0\n",
    "\n",
    "        # Starting training from here\n",
    "        train_writer = tf.summary.FileWriter(os.getcwd() + '/logs', graph=sess.graph)\n",
    "        print (\"Starting Training\")\n",
    "        for global_step in range(start_epoch,EPOCHS):\n",
    "            shoe_batch = get_next_batch(BATCH_SIZE,\"shoes\")\n",
    "            bag_batch = get_next_batch(BATCH_SIZE,\"bags\")\n",
    "            feed_dict_batch = {model.X_bags: bag_batch, model.X_shoes: shoe_batch}\n",
    "            op_list = [model.disc_optimizer, model.gen_optimizer, model.disc_loss, model.gen_loss, model.summary_]\n",
    "            _, _, disc_loss, gen_loss, summary_ = sess.run(op_list, feed_dict=feed_dict_batch)\n",
    "            shoe_batch = get_next_batch(BATCH_SIZE, \"shoes\")\n",
    "            bag_batch = get_next_batch(BATCH_SIZE, \"bags\")\n",
    "            feed_dict_batch = {model.X_bags: bag_batch, model.X_shoes: shoe_batch}\n",
    "            _, gen_loss = sess.run([model.gen_optimizer, model.gen_loss], feed_dict=feed_dict_batch)\n",
    "            if global_step%10 ==0:\n",
    "                train_writer.add_summary(summary_,global_step)\n",
    "\n",
    "            if global_step%100 == 0:\n",
    "                print(\"EPOCH:\" + str(global_step) + \"\\tGenerator Loss: \" + str(gen_loss) + \"\\tDiscriminator Loss: \" + str(disc_loss))\n",
    "\n",
    "\n",
    "            if global_step % 1000 == 0:\n",
    "\n",
    "                shoe_sample = get_next_batch(1, \"shoes\")\n",
    "                bag_sample = get_next_batch(1, \"bags\")\n",
    "\n",
    "                ops = [model.gen_s_fake, model.gen_b_fake, model.gen_recon_s, model.gen_recon_b]\n",
    "                gen_s_fake, gen_b_fake, gen_recon_s, gen_recon_b = sess.run(ops, feed_dict={model.X_shoes: shoe_sample, model.X_bags: bag_sample})\n",
    "\n",
    "                save_image(global_step, gen_s_fake, str(\"gen_s_fake_\") + str(global_step))\n",
    "                save_image(global_step,gen_b_fake, str(\"gen_b_fake_\") + str(global_step))\n",
    "                save_image(global_step, gen_recon_s, str(\"gen_recon_s_\") + str(global_step))\n",
    "                save_image(global_step, gen_recon_b, str(\"gen_recon_b_\") + str(global_step))\n",
    "\n",
    "            if global_step % 1000 == 0:\n",
    "                if not os.path.exists(\"./model\"):\n",
    "                    os.makedirs(\"./model\")\n",
    "                saver.save(sess, \"./model\" + '/model-' + str(global_step) + '.ckpt')\n",
    "                print(\"Saved Model\")\n",
    "\n",
    "def main():\n",
    "    # Get the dataset first.\n",
    "\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), \"bags\")):\n",
    "        print(\"Generating Dataset\")\n",
    "        generate_dataset()\n",
    "    # Create the model\n",
    "    print (\"Defining the model\")\n",
    "    model = DiscoGAN()\n",
    "    print (\"Training\")\n",
    "    train(model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
