{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 0.001 # Learning Rate\n",
    "BATCH_SIZE = 128 # Batch Size\n",
    "CHECKPOINT_PATH_DIR = './datasetslib/model_dir'\n",
    "RESTORE_TRAINING=False\n",
    "SAVE_DIR = './datasetslib/save'\n",
    "\n",
    "# Network Parameters\n",
    "RNN_SIZE = 128 # RNN Size\n",
    "SEQ_LENGTH = 32  # Sequence Length\n",
    "\n",
    "# Data Parameters\n",
    "TEXT_SAVE_DIR= \"./datasetslib/data/postgre_book.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loading Data\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(TEXT_SAVE_DIR)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "def preprocess_and_save_data():\n",
    "    \"\"\"\n",
    "    Preprocessing the Book Scripts Dataset\n",
    "    \"\"\"\n",
    "    text = load_data()\n",
    "    token_dict = define_tokens()\n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    vocab_to_int, int_to_vocab = create_map(text)\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('processed_text.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_preprocess_file():\n",
    "    \"\"\"\n",
    "    Loading the processed Book Scripts Data\n",
    "    \"\"\"\n",
    "    return pickle.load(open('processed_text.p', mode='rb'))\n",
    "\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Saving parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('parameters.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Loading parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('parameters.p', mode='rb'))\n",
    "\n",
    "def create_map(input_text):\n",
    "    \"\"\"\n",
    "    Map words in vocab to int and vice versa for easy lookup\n",
    "    :param input_text: Book Script data split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    vocab = set(input_text)\n",
    "    vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def define_tokens():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token. Note that Sym before each text denotes Symbol\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    dict = {'.':'_Sym_Period_',\n",
    "            ',':'_Sym_Comma_',\n",
    "            '\"':'_Sym_Quote_',\n",
    "            ';':'_Sym_Semicolon_',\n",
    "            '!':'_Sym_Exclamation_',\n",
    "            '?':'_Sym_Question_',\n",
    "            '(':'_Sym_Left_Parentheses_',\n",
    "            ')':'_Sym_Right_Parentheses_',\n",
    "            '--':'_Sym_Dash_',\n",
    "            '\\n':'_Sym_Return_',\n",
    "           }\n",
    "    return dict\n",
    "\n",
    "def generate_batch_data(int_text):\n",
    "    \"\"\"\n",
    "    Generate batch data of x (inputs) and y (targets)\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    num_batches = len(int_text) // (BATCH_SIZE * SEQ_LENGTH)\n",
    "\n",
    "    x = np.array(int_text[:num_batches * (BATCH_SIZE * SEQ_LENGTH)])\n",
    "    y = np.array(int_text[1:num_batches * (BATCH_SIZE * SEQ_LENGTH) + 1])\n",
    "\n",
    "    x_batches = np.split(x.reshape(BATCH_SIZE, -1), num_batches, 1)\n",
    "    y_batches = np.split(y.reshape(BATCH_SIZE, -1), num_batches, 1)\n",
    "    batches = np.array(list(zip(x_batches, y_batches)))\n",
    "    return batches\n",
    "\n",
    "def extract_tensors(tf_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from the graph\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (tensor_input,tensor_initial_state,tensor_final_state, tensor_probs)\n",
    "    \"\"\"\n",
    "    tensor_input = tf_graph.get_tensor_by_name(\"Input/input:0\")\n",
    "    tensor_initial_state = tf_graph.get_tensor_by_name(\"Network/initial_state:0\")\n",
    "    tensor_final_state = tf_graph.get_tensor_by_name(\"Network/final_state:0\")\n",
    "    tensor_probs = tf_graph.get_tensor_by_name(\"Network/probs:0\")\n",
    "    return tensor_input, tensor_initial_state, tensor_final_state, tensor_probs\n",
    "\n",
    "def select_next_word(probs, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Select the next work for the generated text\n",
    "    :param probs: list of probabilities of all the words in vocab which can be selected as next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: predicted next word\n",
    "    \"\"\"\n",
    "    index = np.argmax(probs)\n",
    "    word = int_to_vocab[index]\n",
    "    return word\n",
    "\n",
    "\n",
    "def predict_book_script():\n",
    "    _, vocab_to_int, int_to_vocab, token_dict = load_preprocess_file()\n",
    "    seq_length, load_dir = load_params()\n",
    "\n",
    "    script_length = 250 # Length of Book script to generate. 250 denotes 250 words\n",
    "\n",
    "    first_word = 'postgresql' # postgresql or any other word from the book\n",
    "\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        input_text, initial_state, final_state, probs = extract_tensors(loaded_graph)\n",
    "\n",
    "        # Sentences generation setup\n",
    "        sentences = [first_word]\n",
    "        previous_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "        # Generate sentences\n",
    "        for i in range(script_length):\n",
    "            # Dynamic Input\n",
    "            dynamic_input = [[vocab_to_int[word] for word in sentences[-seq_length:]]]\n",
    "            dynamic_seq_length = len(dynamic_input[0])\n",
    "\n",
    "            # Get Prediction\n",
    "            probabilities, previous_state = sess.run([probs, final_state], {input_text: dynamic_input, initial_state: previous_state})\n",
    "            probabilities= np.squeeze(probabilities)\n",
    "\n",
    "            pred_word = select_next_word(probabilities[dynamic_seq_length - 1], int_to_vocab)\n",
    "            sentences.append(pred_word)\n",
    "\n",
    "        # Scraping out tokens from the words\n",
    "        book_script = ' '.join(sentences)\n",
    "        for key, token in token_dict.items():\n",
    "            book_script = book_script.replace(' ' + token.lower(), key)\n",
    "        book_script = book_script.replace('\\n ', '\\n')\n",
    "        book_script = book_script.replace('( ', '(')\n",
    "\n",
    "        # Write the generated script to a file\n",
    "        with open(\"book_script\", \"w\") as text_file:\n",
    "            text_file.write(book_script)\n",
    "\n",
    "        print(book_script)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, int_to_vocab):\n",
    "        self.vocab_size = len(int_to_vocab)\n",
    "\n",
    "        with tf.variable_scope('Input'):\n",
    "            self.X = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "            self.Y = tf.placeholder(tf.int32, [None, None], name='target')\n",
    "            self.input_shape = tf.shape(self.X)\n",
    "\n",
    "        self.define_network()\n",
    "        self.define_loss()\n",
    "        self.define_optimizer()\n",
    "\n",
    "    def define_network(self):\n",
    "        # Define an init cell of RNN\n",
    "        with tf.variable_scope(\"Network\"):\n",
    "            # Defining an initial cell state\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(RNN_SIZE)\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([lstm] * 2)  # Defining two LSTM layers for this case\n",
    "            self.initial_state = cell.zero_state(self.input_shape[0], tf.float32)\n",
    "            self.initial_state = tf.identity(self.initial_state, name=\"initial_state\")\n",
    "\n",
    "            embedding = tf.Variable(tf.random_uniform((self.vocab_size, RNN_SIZE), -1, 1))\n",
    "            embed = tf.nn.embedding_lookup(embedding, self.X)\n",
    "\n",
    "            outputs, self.final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=None, dtype=tf.float32)\n",
    "            self.final_state = tf.identity(self.final_state, name='final_state')\n",
    "            self.predictions = tf.contrib.layers.fully_connected(outputs, self.vocab_size, activation_fn=None)\n",
    "            # Probabilities for generating words\n",
    "            probs = tf.nn.softmax(self.predictions, name='probs')\n",
    "\n",
    "    def define_loss(self):\n",
    "        # Defining the sequence loss\n",
    "        with tf.variable_scope('Sequence_Loss'):\n",
    "            self.loss = seq2seq.sequence_loss(self.predictions, self.Y,\n",
    "                                              tf.ones([self.input_shape[0], self.input_shape[1]]))\n",
    "\n",
    "    def define_optimizer(self):\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "            # Gradient Clipping\n",
    "            gradients = optimizer.compute_gradients(self.loss)\n",
    "            capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(capped_gradients)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the data\n",
      "Loading the preprocessed data\n",
      "WARNING:tensorflow:From <ipython-input-3-34c75a0d484a>:21: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-3-34c75a0d484a>:22: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-3-34c75a0d484a>:29: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Training the model\n",
      "All variables initialized\n",
      "Epoch   0 Batch    0/12   train_loss = 8.183\n",
      "Epoch  16 Batch    8/12   train_loss = 6.058\n",
      "Epoch  33 Batch    4/12   train_loss = 5.952\n",
      "Epoch  50 Batch    0/12   train_loss = 5.853\n",
      "Epoch  66 Batch    8/12   train_loss = 5.604\n",
      "Epoch  83 Batch    4/12   train_loss = 5.155\n",
      "Epoch 100 Batch    0/12   train_loss = 4.903\n",
      "Epoch 116 Batch    8/12   train_loss = 4.700\n",
      "Epoch 133 Batch    4/12   train_loss = 4.352\n",
      "Epoch 150 Batch    0/12   train_loss = 4.198\n",
      "Epoch 166 Batch    8/12   train_loss = 4.039\n",
      "Epoch 183 Batch    4/12   train_loss = 3.734\n",
      "Epoch 200 Batch    0/12   train_loss = 3.633\n",
      "Epoch 216 Batch    8/12   train_loss = 3.467\n",
      "Epoch 233 Batch    4/12   train_loss = 3.204\n",
      "Epoch 250 Batch    0/12   train_loss = 3.185\n",
      "Epoch 266 Batch    8/12   train_loss = 2.955\n",
      "Epoch 283 Batch    4/12   train_loss = 2.729\n",
      "Epoch 300 Batch    0/12   train_loss = 2.655\n",
      "Epoch 316 Batch    8/12   train_loss = 2.538\n",
      "Epoch 333 Batch    4/12   train_loss = 2.309\n",
      "Epoch 350 Batch    0/12   train_loss = 2.222\n",
      "Epoch 366 Batch    8/12   train_loss = 2.149\n",
      "Epoch 383 Batch    4/12   train_loss = 1.985\n",
      "Epoch 400 Batch    0/12   train_loss = 1.899\n",
      "Epoch 416 Batch    8/12   train_loss = 1.803\n",
      "Epoch 433 Batch    4/12   train_loss = 1.669\n",
      "Epoch 450 Batch    0/12   train_loss = 1.759\n",
      "Epoch 466 Batch    8/12   train_loss = 1.578\n",
      "Epoch 483 Batch    4/12   train_loss = 1.416\n",
      "Model Trained and Saved\n",
      "Generating the Book Script\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./datasetslib/save\n",
      "postgresql relative under will read. which will be used in a random column, this is not to detect anyway.\n",
      "postgresql are lowercase. however, if you are also using, you have got to be read.\n",
      "to use the following line the same, which will be rejected.\n",
      "in this section, you will learn what you have to be an way to do. in this case, you will be able to run the most important topics. it can happen that something allows you to do exactly that. as you can see, the table is what does not know whether a function is passed to the server, 000 times. the following example shows how more allows the end to the database, which is the same as follows\n",
      "so far, the following example shows how more information about the table, i have written a millisecond, and how in this book has to be kept as the string.\n",
      "to show how things work, a simple table is what, there is no way to be provided\n",
      "\n",
      "\n",
      "as you can see, postgresql will see what really types. if you are also using various types of rows, postgresql has to be stored for a transaction time to make your applications can be slower because to configure the postgresql.\n",
      "\n",
      "as you can see\n"
     ]
    }
   ],
   "source": [
    "def train(model,int_text):\n",
    "\n",
    "    # Creating the checkpoint directory\n",
    "    if not os.path.exists(CHECKPOINT_PATH_DIR):\n",
    "        os.makedirs(CHECKPOINT_PATH_DIR)\n",
    "\n",
    "    batches = generate_batch_data(int_text)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        if RESTORE_TRAINING:\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(CHECKPOINT_PATH_DIR)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('Model Loaded')\n",
    "            start_epoch = int(str(ckpt.model_checkpoint_path).split('-')[-1])\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('All variables initialized')\n",
    "\n",
    "        for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "            saver = tf.train.Saver()\n",
    "            state = sess.run(model.initial_state, {model.X: batches[0][0]})\n",
    "\n",
    "            for batch, (x, y) in enumerate(batches):\n",
    "                feed = {\n",
    "                    model.X: x,\n",
    "                    model.Y: y,\n",
    "                    model.initial_state: state}\n",
    "                train_loss, state, _ = sess.run([model.loss, model.final_state, model.train_op], feed)\n",
    "\n",
    "                if (epoch * len(batches) + batch) % 200 == 0:\n",
    "                    print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                        epoch,\n",
    "                        batch,\n",
    "                        len(batches),\n",
    "                        train_loss))\n",
    "                    # Save Checkpoint for restoring if required\n",
    "                    saver.save(sess, CHECKPOINT_PATH_DIR + '/model.tfmodel', global_step=epoch + 1)\n",
    "\n",
    "        # Save Model\n",
    "        saver.save(sess, SAVE_DIR)\n",
    "        print('Model Trained and Saved')\n",
    "        save_params((SEQ_LENGTH, SAVE_DIR))\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    if os.path.exists(\"./processed_text.p\"):\n",
    "        print (\"Processed File Already Present. Proceeding with that\")\n",
    "    else:\n",
    "        print (\"Preprocessing the data\")\n",
    "        preprocess_and_save_data()\n",
    "\n",
    "    print (\"Loading the preprocessed data\")\n",
    "    int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess_file()\n",
    "\n",
    "    model = Model(int_to_vocab)\n",
    "    print (\"Training the model\")\n",
    "    train(model,int_text)\n",
    "\n",
    "    print (\"Generating the Book Script\")\n",
    "    predict_book_script()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
