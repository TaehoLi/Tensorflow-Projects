{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data from here: https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "##### Arguments ####\n",
    "\n",
    "## Fashion MNIST Parameters\n",
    "N_CLASSES = 10\n",
    "IMG_WIDTH = 28\n",
    "IMG_HEIGHT = 28\n",
    "N_CHANNELS = 1                 # Number of Input Channels\n",
    "IMAGE_LABELS = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "\n",
    "\n",
    "## Model Parameters\n",
    "CONV1_LAYER_PARAMS =  {\"filters\": 256,\n",
    "                \"kernel_size\": 9,\n",
    "                \"activation\": tf.nn.relu,\n",
    "                \"padding\": \"valid\",\n",
    "                \"strides\": 1\n",
    "                }\n",
    "\n",
    "# Parameters of PrimaryCaps_layer\n",
    "MAPS_CAPS1 = 32\n",
    "NCAPS_CAPS1 = MAPS_CAPS1*6*6  # Total number of primary capsules = 1152\n",
    "CAPS_DIM_CAPS1 = 8            # Dimensions of each capsule\n",
    "\n",
    "CONV2_LAYER_PARAMS  = {\"filters\": MAPS_CAPS1 * CAPS_DIM_CAPS1,  # Total Convolutional Filters = 256\n",
    "                \"kernel_size\": 9,\n",
    "                \"strides\": 2,\n",
    "                \"padding\": \"valid\",\n",
    "                \"activation\": tf.nn.relu}\n",
    "\n",
    "# Parameters of DigitCaps_layer\n",
    "NCAPS_CAPS2 = 10\n",
    "CAPS_DIM_CAPS2 = 16           # Dimension of each capsule in layer 2\n",
    "\n",
    "# Decoder Parameters\n",
    "layer1_size = 512\n",
    "layer2_size = 1024\n",
    "output_size = IMG_WIDTH* IMG_HEIGHT\n",
    "\n",
    "## Loss\n",
    "\n",
    "# Margin Loss\n",
    "M_PLUS = 0.9\n",
    "M_MINUS= 0.1\n",
    "LAMBDA = 0.5\n",
    "\n",
    "# Reconstruction Loss\n",
    "ALPHA = 0.0005\n",
    "\n",
    "# Training Params\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "ROUTING_ITERATIONS = 3    # Routing Iterations\n",
    "STDEV = 0.01  # STDEV for Weight Initialization\n",
    "\n",
    "\n",
    "## Environment and Save Directories\n",
    "RESTORE_TRAINING = False            # Restores the trained model\n",
    "CHECKPOINT_PATH_DIR = './datasetslib/model_dir'\n",
    "LOG_DIR = './datasetslib/logs'\n",
    "RESULTS_DIR = './datasetslib/results'\n",
    "STEPS_TO_SAVE = 100                 # Frequency (in steps) of saving the train result\n",
    "\n",
    "## Visualization Parameters\n",
    "N_SAMPLES = 3                       # No. of Samples Images to Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "## Functions for Capsule Networks\n",
    "def squash(vectors, name=None):\n",
    "    \"\"\"\n",
    "    Squashing Function as implemented in the paper\n",
    "    :parameter vectors: vector input that needs to be squashed\n",
    "    :parameter name: Name of the tensor on the graph\n",
    "    :return: a tensor with same shape as vectors but squashed as mentioned in the paper\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, default_name=\"squash_op\"):\n",
    "        s_squared_norm = tf.reduce_sum(tf.square(vectors), axis=-2, keepdims=True)\n",
    "        scale = s_squared_norm / (1. + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
    "        return scale*vectors\n",
    "\n",
    "\n",
    "def routing(u):\n",
    "    \"\"\"\n",
    "    This function performs the routing algorithm as mentioned in the paper\n",
    "    :parameter u: Input tensor with [batch_size, num_caps_input_layer=1152, 1, caps_dim_input_layer=8, 1] shape.\n",
    "                NCAPS_CAPS1: num capsules in the PrimaryCaps layer l\n",
    "                CAPS_DIM_CAPS2: dimensions of output vectors of Primary caps layer l\n",
    "\n",
    "    :return: \"v_j\" vector (tensor) in Digitcaps Layer\n",
    "             Shape:[batch_size, NCAPS_CAPS1=10, CAPS_DIM_CAPS2=16, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    #local variable b_ij: [batch_size, num_caps_input_layer=1152, num_caps_output_layer=10, 1, 1]\n",
    "                #num_caps_output_layer: number of capsules in Digicaps layer l+1\n",
    "    b_ij = tf.zeros([BATCH_SIZE, NCAPS_CAPS1, NCAPS_CAPS2, 1, 1], dtype=np.float32, name=\"b_ij\")\n",
    "\n",
    "    # Preparing the input Tensor for total number of DigitCaps capsule for multiplication with W\n",
    "    u = tf.tile(u, [1, 1, b_ij.shape[2].value, 1, 1])   # u => [batch_size, 1152, 10, 8, 1]\n",
    "\n",
    "\n",
    "    # W: [num_caps_input_layer, num_caps_output_layer, len_u_i, len_v_j] as mentioned in the paper\n",
    "    W = tf.get_variable('W', shape=(1, u.shape[1].value, b_ij.shape[2].value, u.shape[3].value, CAPS_DIM_CAPS2),\n",
    "                        dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=STDEV))\n",
    "    W = tf.tile(W, [BATCH_SIZE, 1, 1, 1, 1]) # W => [batch_size, 1152, 10, 8, 16]\n",
    "\n",
    "    #Computing u_hat (as mentioned in the paper)\n",
    "    u_hat = tf.matmul(W, u, transpose_a=True)  # [batch_size, 1152, 10, 16, 1]\n",
    "\n",
    "    # In forward, u_hat_stopped = u_hat;\n",
    "    # In backward pass, no gradient pass from  u_hat_stopped to u_hat\n",
    "    u_hat_stopped = tf.stop_gradient(u_hat, name='gradient_stop')\n",
    "\n",
    "    # Routing Algorithm Begins here\n",
    "    for r in range(ROUTING_ITERATIONS):\n",
    "        with tf.variable_scope('iterations_' + str(r)):\n",
    "            c_ij = tf.nn.softmax(b_ij, axis=2) # [batch_size, 1152, 10, 1, 1]\n",
    "\n",
    "            # At last iteration, use `u_hat` in order to back propagate gradient\n",
    "            if r == ROUTING_ITERATIONS - 1:\n",
    "                s_j = tf.multiply(c_ij, u_hat) # [batch_size, 1152, 10, 16, 1]\n",
    "                # then sum as per paper\n",
    "                s_j = tf.reduce_sum(s_j, axis=1, keep_dims=True) # [batch_size, 1, 10, 16, 1]\n",
    "\n",
    "                v_j = squash(s_j) # [batch_size, 1, 10, 16, 1]\n",
    "\n",
    "            elif r < ROUTING_ITERATIONS - 1:  # No backpropagation in these iterations\n",
    "                s_j = tf.multiply(c_ij, u_hat_stopped)\n",
    "                s_j = tf.reduce_sum(s_j, axis=1, keepdims=True)\n",
    "                v_j = squash(s_j)\n",
    "                v_j = tf.tile(v_j, [1, u.shape[1].value, 1, 1, 1]) # [batch_size, 1152, 10, 16, 1]\n",
    "\n",
    "                # Multiplying in last two dimensions: [16, 1]^T x [16, 1] yields [1, 1]\n",
    "                u_hat_dot_v = tf.matmul(u_hat_stopped, v_j, transpose_a=True) # [batch_size, 1152, 10, 1, 1]\n",
    "\n",
    "                b_ij = tf.add(b_ij,u_hat_dot_v)\n",
    "    return tf.squeeze(v_j, axis=1) # [batch_size, 10, 16, 1]\n",
    "\n",
    "\n",
    "\n",
    "def load_data(load_type='train'):\n",
    "    '''\n",
    "\n",
    "    :param load_type: train or test depending on the use case\n",
    "    :return: x (images), y(labels)\n",
    "    '''\n",
    "    data_dir = os.path.join(os.getcwd(), 'datasetslib', 'data','fashion-mnist')\n",
    "    if load_type == 'train':\n",
    "        image_file = open(os.path.join(data_dir,'train-images-idx3-ubyte'))\n",
    "        image_data = np.fromfile(file=image_file, dtype=np.uint8)\n",
    "        x = image_data[16:].reshape((60000, 28, 28, 1)).astype(np.float32)\n",
    "\n",
    "        label_file = open(os.path.join(data_dir, 'train-labels-idx1-ubyte'))\n",
    "        label_data = np.fromfile(file=label_file, dtype=np.uint8)\n",
    "        y = label_data[8:].reshape(60000).astype(np.int32)\n",
    "\n",
    "        x_train = x[:55000] / 255.\n",
    "        y_train = y[:55000]\n",
    "        y_train = (np.arange(N_CLASSES) == y_train[:, None]).astype(np.float32)\n",
    "\n",
    "        x_valid = x[55000:, ] / 255.\n",
    "        y_valid = y[55000:]\n",
    "        y_valid = (np.arange(N_CLASSES) == y_valid[:, None]).astype(np.float32)\n",
    "        return x_train, y_train, x_valid, y_valid\n",
    "    elif load_type == 'test':\n",
    "        image_file = open(os.path.join(data_dir, 't10k-images-idx3-ubyte'))\n",
    "        image_data = np.fromfile(file=image_file, dtype=np.uint8)\n",
    "        x_test = image_data[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n",
    "\n",
    "        label_file = open(os.path.join(data_dir, 't10k-labels-idx1-ubyte'))\n",
    "        label_data = np.fromfile(file=label_file, dtype=np.uint8)\n",
    "        y_test = label_data[8:].reshape(10000).astype(np.int32)\n",
    "        y_test = (np.arange(N_CLASSES) == y_test[:, None]).astype(np.float32)\n",
    "        return x_test / 255., y_test\n",
    "\n",
    "\n",
    "def shuffle_data(x, y):\n",
    "    \"\"\" Shuffle the features and labels of input data\"\"\"\n",
    "    perm = np.arange(y.shape[0])\n",
    "    np.random.shuffle(perm)\n",
    "    shuffle_x = x[perm,:,:,:]\n",
    "    shuffle_y = y[perm]\n",
    "    return shuffle_x, shuffle_y\n",
    "\n",
    "def write_progress(op_type = 'train'):\n",
    "    \"\"\"\n",
    "    Creating the handles for saving the results in a .csv file\n",
    "    :return: appropriate logging files\n",
    "    \"\"\"\n",
    "    if not os.path.exists(RESULTS_DIR):\n",
    "        os.mkdir(RESULTS_DIR)\n",
    "    if op_type == 'train':\n",
    "        train_path = RESULTS_DIR  + '/' + 'train.csv'\n",
    "        val_path = RESULTS_DIR + '/' + 'validation.csv'\n",
    "\n",
    "        if os.path.exists(train_path):\n",
    "            os.remove(train_path)\n",
    "        if os.path.exists(val_path):\n",
    "            os.remove(val_path)\n",
    "\n",
    "        train_file = open(train_path, 'w')\n",
    "        train_file.write('step,accuracy,loss\\n')\n",
    "        val_file = open(val_path, 'w')\n",
    "        val_file.write('epoch,accuracy,loss\\n')\n",
    "        return train_file, val_file\n",
    "    else:\n",
    "        test_path = RESULTS_DIR + '/test.csv'\n",
    "        if os.path.exists(test_path):\n",
    "            os.remove(test_path)\n",
    "        test_file = open(test_path, 'w')\n",
    "        test_file.write('accuracy,loss\\n')\n",
    "        return test_file\n",
    "\n",
    "\n",
    "def load_existing_details():\n",
    "    \"\"\"\n",
    "    This function loads the train and val files to continue training\n",
    "    :return: handles to train and val files and minimum validation loss\n",
    "    \"\"\"\n",
    "    train_path = RESULTS_DIR  + '/' + 'train.csv'\n",
    "    val_path = RESULTS_DIR + '/' + 'validation.csv'\n",
    "    # finding the minimum validation loss so far\n",
    "    f_val = open(val_path, 'r')\n",
    "    lines = f_val.readlines()\n",
    "    data = np.genfromtxt(lines[-1:], delimiter=',')\n",
    "    min_loss = np.min(data[1:, 2])\n",
    "    # loading the train and val files to continue training\n",
    "    train_file = open(train_path, 'a')\n",
    "    val_file = open(val_path, 'a')\n",
    "    return train_file, val_file, min_loss\n",
    "\n",
    "\n",
    "def eval_performance(sess, model, x, y):\n",
    "    '''\n",
    "    This function is mainly used to evaluate the accuracy on test and validation sets\n",
    "    :param sess: session\n",
    "    :param model: model to be used\n",
    "    :param x: images\n",
    "    :param y: labels\n",
    "    :return: returns the average accuracy and loss for the dataset\n",
    "    '''\n",
    "    acc_all = loss_all = np.array([])\n",
    "    num_batches = int(y.shape[0] / BATCH_SIZE)\n",
    "    for batch_num in range(num_batches):\n",
    "        start = batch_num * BATCH_SIZE\n",
    "        end = start + BATCH_SIZE\n",
    "        x_batch, y_batch = x[start:end], y[start:end]\n",
    "        acc_batch, loss_batch, prediction_batch = sess.run([model.accuracy, model.combined_loss, model.y_predicted],\n",
    "                                                     feed_dict={model.X: x_batch, model.Y: y_batch})\n",
    "        acc_all = np.append(acc_all, acc_batch)\n",
    "        loss_all = np.append(loss_all, loss_batch)\n",
    "    return np.mean(acc_all), np.mean(loss_all)\n",
    "\n",
    "def reconstruction(x, y, decoder_output, y_pred, n_samples):\n",
    "    '''\n",
    "    This function is used to reconstruct sample images for analysis\n",
    "    :param x: Images\n",
    "    :param y: Labels\n",
    "    :param decoder_output: output from decoder\n",
    "    :param y_pred: predictions from the model\n",
    "    :param n_samples: num images\n",
    "    :return: saves the reconstructed images\n",
    "    '''\n",
    "\n",
    "    sample_images = x.reshape(-1, IMG_WIDTH, IMG_HEIGHT)\n",
    "    decoded_image = decoder_output.reshape([-1, IMG_WIDTH, IMG_WIDTH])\n",
    "\n",
    "    fig = plt.figure(figsize=(n_samples * 2, 3))\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(1, n_samples, i+ 1)\n",
    "        plt.imshow(sample_images[i], cmap=\"binary\")\n",
    "        plt.title(\"Label:\" + IMAGE_LABELS[np.argmax(y[i])])\n",
    "        plt.axis(\"off\")\n",
    "    fig.savefig(RESULTS_DIR + '/' + 'input_images.png')\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure(figsize=(n_samples * 2, 3))\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(1, n_samples, i + 1)\n",
    "        plt.imshow(decoded_image[i], cmap=\"binary\")\n",
    "        plt.title(\"Prediction:\" + IMAGE_LABELS[y_pred[i]])\n",
    "        plt.axis(\"off\")\n",
    "    fig.savefig(RESULTS_DIR + '/' + 'decoder_images.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build capsule networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet:\n",
    "    def __init__(self):\n",
    "        with tf.variable_scope('Input'):\n",
    "            self.X = tf.placeholder(shape=[None, IMG_WIDTH, IMG_HEIGHT, N_CHANNELS], dtype=tf.float32, name=\"X\")\n",
    "            self.Y = tf.placeholder(shape=[None, N_CLASSES], dtype=tf.float32, name=\"Y\")\n",
    "            self.mask_with_labels = tf.placeholder_with_default(False, shape=(), name=\"mask_with_labels\")\n",
    "\n",
    "        self.define_network()\n",
    "        self.define_loss()\n",
    "        self.define_accuracy()\n",
    "        self.define_optimizer()\n",
    "        self.summary_()\n",
    "\n",
    "    def define_network(self):\n",
    "        with tf.variable_scope('Conv1_layer'):\n",
    "            conv1_layer = tf.layers.conv2d(self.X, name=\"conv1_layer\", **CONV1_LAYER_PARAMS) # [batch_size, 20, 20, 256]\n",
    "\n",
    "        with tf.variable_scope('PrimaryCaps_layer'):\n",
    "            conv2_layer = tf.layers.conv2d(conv1_layer, name=\"conv2_layer\", **CONV2_LAYER_PARAMS) # [batch_size, 6, 6, 256]\n",
    "\n",
    "            primary_caps = tf.reshape(conv2_layer, (BATCH_SIZE, NCAPS_CAPS1, CAPS_DIM_CAPS1, 1), name=\"primary_caps\") # [batch_size, 1152, 8, 1]\n",
    "            primary_caps_output = squash(primary_caps, name=\"caps1_output\")\n",
    "            # [batch_size, 1152, 8, 1]\n",
    "\n",
    "        # DigitCaps layer, return [batch_size, 10, 16, 1]\n",
    "        with tf.variable_scope('DigitCaps_layer'):\n",
    "            digitcaps_input = tf.reshape(primary_caps_output, shape=(BATCH_SIZE, NCAPS_CAPS1, 1, CAPS_DIM_CAPS1, 1)) # [batch_size, 1152, 1, 8, 1]\n",
    "            # [batch_size, 1152, 10, 1, 1]\n",
    "            self.digitcaps_output = routing(digitcaps_input) # [batch_size, 10, 16, 1]\n",
    "\n",
    "        # Decoder\n",
    "        with tf.variable_scope('Masking'):\n",
    "            self.v_norm = tf.sqrt(tf.reduce_sum(tf.square(self.digitcaps_output), axis=2, keep_dims=True) + tf.keras.backend.epsilon())\n",
    "\n",
    "            predicted_class = tf.to_int32(tf.argmax(self.v_norm, axis=1)) #[batch_size, 10,1,1]\n",
    "            self.y_predicted = tf.reshape(predicted_class, shape=(BATCH_SIZE,))  #[batch_size]\n",
    "            y_predicted_one_hot = tf.one_hot(self.y_predicted, depth=NCAPS_CAPS2)  #[batch_size,10]  One hot operation\n",
    "\n",
    "            reconstruction_targets = tf.cond(self.mask_with_labels,  # condition\n",
    "                                      lambda: self.Y,  # if True (Training)\n",
    "                                      lambda: y_predicted_one_hot,  # if False (Test)\n",
    "                                      name=\"reconstruction_targets\")\n",
    "\n",
    "            digitcaps_output_masked = tf.multiply(tf.squeeze(self.digitcaps_output), tf.expand_dims(reconstruction_targets, -1)) # [batch_size, 10, 16]\n",
    "\n",
    "\n",
    "            #Flattening as suggested by the paper\n",
    "            decoder_input = tf.reshape(digitcaps_output_masked, [BATCH_SIZE, -1]) # [batch_size, 160]\n",
    "\n",
    "\n",
    "        with tf.variable_scope('Decoder'):\n",
    "            fc1 = tf.layers.dense(decoder_input, layer1_size, activation=tf.nn.relu, name=\"FC1\") # [batch_size, 512]\n",
    "            fc2 = tf.layers.dense(fc1, layer2_size, activation=tf.nn.relu, name=\"FC2\") # [batch_size, 1024]\n",
    "            self.decoder_output = tf.layers.dense(fc2, output_size, activation=tf.nn.sigmoid, name=\"FC3\") # [batch_size, 784]\n",
    "\n",
    "\n",
    "    def define_loss(self):\n",
    "        # Margin Loss\n",
    "        with tf.variable_scope('Margin_Loss'):\n",
    "            # max(0, m_plus-||v_c||)^2\n",
    "            positive_error = tf.square(tf.maximum(0., 0.9 - self.v_norm)) # [batch_size, 10, 1, 1]\n",
    "            # max(0, ||v_c||-m_minus)^2\n",
    "            negative_error = tf.square(tf.maximum(0., self.v_norm - 0.1)) # [batch_size, 10, 1, 1]\n",
    "            # reshape: [batch_size, 10, 1, 1] => [batch_size, 10]\n",
    "            positive_error = tf.reshape(positive_error, shape=(BATCH_SIZE, -1))\n",
    "            negative_error = tf.reshape(negative_error, shape=(BATCH_SIZE, -1))\n",
    "\n",
    "            Loss_vec = self.Y * positive_error + 0.5 * (1- self.Y) * negative_error # [batch_size, 10]\n",
    "            self.margin_loss = tf.reduce_mean(tf.reduce_sum(Loss_vec, axis=1), name=\"margin_loss\")\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        with tf.variable_scope('Reconstruction_Loss'):\n",
    "            ground_truth = tf.reshape(self.X, shape=(BATCH_SIZE, -1))\n",
    "            self.reconstruction_loss = tf.reduce_mean(tf.square(self.decoder_output - ground_truth))\n",
    "\n",
    "        # Combined Loss\n",
    "        with tf.variable_scope('Combined_Loss'):\n",
    "            self.combined_loss = self.margin_loss + 0.0005 * self.reconstruction_loss\n",
    "\n",
    "    def define_accuracy(self):\n",
    "        with tf.variable_scope('Accuracy'):\n",
    "            correct_predictions = tf.equal(tf.to_int32(tf.argmax(self.Y, axis=1)), self.y_predicted)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "\n",
    "    def define_optimizer(self):\n",
    "        with tf.variable_scope('Optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            self.train_optimizer = optimizer.minimize(self.combined_loss, name=\"training_optimizer\")\n",
    "\n",
    "    def summary_(self):\n",
    "        reconstructed_image = tf.reshape(self.decoder_output, shape=(BATCH_SIZE, IMG_WIDTH, IMG_HEIGHT, N_CHANNELS))\n",
    "        summary_list = [tf.summary.scalar('Loss/margin_loss', self.margin_loss),\n",
    "                        tf.summary.scalar('Loss/reconstruction_loss', self.reconstruction_loss),\n",
    "                        tf.summary.image('original', self.X),\n",
    "                        tf.summary.image('reconstructed', reconstructed_image)]\n",
    "        self.summary_ = tf.summary.merge(summary_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-5f5d1f50476c>:16: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-2-59cda6775c90>:61: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-3-5f5d1f50476c>:35: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-5f5d1f50476c>:52: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "Step1: Train\n",
      "Data set Loaded\n",
      "All variables initialized\n",
      "Training Starts\n",
      "  Batch #0, Epoch: #1, Mean Training loss: 0.5638, Mean Training accuracy: 8.6%\n",
      "  Batch #100, Epoch: #1, Mean Training loss: 0.8095, Mean Training accuracy: 9.8%\n",
      "  Batch #200, Epoch: #1, Mean Training loss: 0.8095, Mean Training accuracy: 9.8%\n",
      "  Batch #300, Epoch: #1, Mean Training loss: 0.8095, Mean Training accuracy: 10.0%\n",
      "  Batch #400, Epoch: #1, Mean Training loss: 0.8095, Mean Training accuracy: 10.3%\n",
      "Epoch: 1  Mean Train Accuracy: 10.3203% ,Mean Val accuracy: 10.5369%  Loss: 1.244087 (improved)\n",
      "  Batch #0, Epoch: #2, Mean Training loss: 0.8086, Mean Training accuracy: 9.9%\n",
      "  Batch #100, Epoch: #2, Mean Training loss: 0.0023, Mean Training accuracy: 10.3%\n",
      "  Batch #200, Epoch: #2, Mean Training loss: 0.0000, Mean Training accuracy: 9.8%\n",
      "  Batch #300, Epoch: #2, Mean Training loss: 0.0000, Mean Training accuracy: 9.7%\n",
      "  Batch #400, Epoch: #2, Mean Training loss: 0.0000, Mean Training accuracy: 9.8%\n",
      "Epoch: 2  Mean Train Accuracy: 9.8281% ,Mean Val accuracy: 10.2163%  Loss: 0.426961 (improved)\n",
      "  Batch #0, Epoch: #3, Mean Training loss: 0.0000, Mean Training accuracy: 10.7%\n",
      "  Batch #100, Epoch: #3, Mean Training loss: 0.0000, Mean Training accuracy: 9.7%\n",
      "  Batch #200, Epoch: #3, Mean Training loss: 0.0000, Mean Training accuracy: 10.4%\n",
      "  Batch #300, Epoch: #3, Mean Training loss: 0.0000, Mean Training accuracy: 9.8%\n",
      "  Batch #400, Epoch: #3, Mean Training loss: 0.0000, Mean Training accuracy: 10.0%\n",
      "Epoch: 3  Mean Train Accuracy: 10.0234% ,Mean Val accuracy: 10.1162%  Loss: 0.347101 (improved)\n",
      "  Batch #0, Epoch: #4, Mean Training loss: 0.0000, Mean Training accuracy: 9.9%\n",
      "  Batch #100, Epoch: #4, Mean Training loss: 0.0000, Mean Training accuracy: 10.1%\n",
      "  Batch #200, Epoch: #4, Mean Training loss: 0.0000, Mean Training accuracy: 9.7%\n",
      "  Batch #300, Epoch: #4, Mean Training loss: 0.0000, Mean Training accuracy: 10.3%\n",
      "  Batch #400, Epoch: #4, Mean Training loss: 0.0000, Mean Training accuracy: 9.8%\n",
      "Epoch: 4  Mean Train Accuracy: 9.7734% ,Mean Val accuracy: 10.1763%  Loss: 0.379668\n",
      "  Batch #0, Epoch: #5, Mean Training loss: 0.0000, Mean Training accuracy: 9.5%\n",
      "  Batch #100, Epoch: #5, Mean Training loss: 0.0000, Mean Training accuracy: 10.2%\n",
      "  Batch #200, Epoch: #5, Mean Training loss: 0.0000, Mean Training accuracy: 9.6%\n",
      "  Batch #300, Epoch: #5, Mean Training loss: 0.0000, Mean Training accuracy: 10.2%\n",
      "  Batch #400, Epoch: #5, Mean Training loss: 0.0000, Mean Training accuracy: 9.7%\n",
      "Epoch: 5  Mean Train Accuracy: 9.7109% ,Mean Val accuracy: 10.2163%  Loss: 0.363044\n",
      "Step2: Testing the performance of model on the Test Set\n",
      "Loaded the test dataset\n",
      "WARNING:tensorflow:From /home/taeho/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./datasetslib/model_dir/model.tfmodel-3\n",
      "Model Loaded\n",
      "-----------------------------------------------------------------------------\n",
      "Test Set Loss: 0.4451, Test Set Accuracy: 10.1%\n",
      "Step3: Reconstructing some sample images\n",
      "INFO:tensorflow:Restoring parameters from ./datasetslib/model_dir/model.tfmodel-3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    global fd_train\n",
    "    x_train, y_train, x_valid, y_valid = load_data(load_type='train')\n",
    "    print('Data set Loaded')\n",
    "    num_batches = int(y_train.shape[0] / BATCH_SIZE)\n",
    "    if not os.path.exists(CHECKPOINT_PATH_DIR):\n",
    "        os.makedirs(CHECKPOINT_PATH_DIR)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        if RESTORE_TRAINING:\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(CHECKPOINT_PATH_DIR)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('Model Loaded')\n",
    "            start_epoch = int(str(ckpt.model_checkpoint_path).split('-')[-1])\n",
    "            train_file, val_file, best_loss_val = load_existing_details()\n",
    "        else:\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('All variables initialized')\n",
    "            train_file, val_file = write_progress('train')\n",
    "            start_epoch = 0\n",
    "            best_loss_val = np.infty\n",
    "        print('Training Starts')\n",
    "        acc_batch_all = loss_batch_all = np.array([])\n",
    "        train_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
    "        for epoch in range(start_epoch, EPOCHS):\n",
    "            # Shuffle the input data\n",
    "            x_train, y_train = shuffle_data(x_train, y_train)\n",
    "            for step in range(num_batches):\n",
    "                start = step * BATCH_SIZE\n",
    "                end = (step + 1) * BATCH_SIZE\n",
    "                global_step = epoch * num_batches + step\n",
    "                x_batch, y_batch = x_train[start:end], y_train[start:end]\n",
    "                feed_dict_batch = {model.X: x_batch, model.Y: y_batch, model.mask_with_labels: True}\n",
    "                if not (step % 100):\n",
    "                    _, acc_batch, loss_batch, summary_ = sess.run([model.train_optimizer, model.accuracy,\n",
    "                                                                     model.combined_loss, model.summary_],\n",
    "                                                                    feed_dict=feed_dict_batch)\n",
    "                    train_writer.add_summary(summary_, global_step)\n",
    "                    acc_batch_all = np.append(acc_batch_all, acc_batch)\n",
    "                    loss_batch_all = np.append(loss_batch_all, loss_batch)\n",
    "                    mean_acc,mean_loss = np.mean(acc_batch_all),np.mean(loss_batch_all)\n",
    "                    summary_ = tf.Summary(value=[tf.Summary.Value(tag='Accuracy', simple_value=mean_acc)])\n",
    "                    train_writer.add_summary(summary_, global_step)\n",
    "                    summary_ = tf.Summary(value=[tf.Summary.Value(tag='Loss/combined_loss', simple_value=mean_loss)])\n",
    "                    train_writer.add_summary(summary_, global_step)\n",
    "\n",
    "                    train_file.write(str(global_step) + ',' + str(mean_acc) + ',' + str(mean_loss) + \"\\n\")\n",
    "                    train_file.flush()\n",
    "                    print(\"  Batch #{0}, Epoch: #{1}, Mean Training loss: {2:.4f}, Mean Training accuracy: {3:.01%}\".format(\n",
    "                        step, (epoch+1), mean_loss, mean_acc))\n",
    "                    acc_batch_all = loss_batch_all = np.array([])\n",
    "                else:\n",
    "                    _, acc_batch, loss_batch = sess.run([model.train_optimizer, model.accuracy, model.combined_loss],\n",
    "                                                        feed_dict=feed_dict_batch)\n",
    "                    acc_batch_all = np.append(acc_batch_all, acc_batch)\n",
    "                    loss_batch_all = np.append(loss_batch_all, loss_batch)\n",
    "\n",
    "            # Validation metrics after each EPOCH\n",
    "            acc_val, loss_val = eval_performance(sess, model, x_valid, y_valid)\n",
    "            val_file.write(str(epoch + 1) + ',' + str(acc_val) + ',' + str(loss_val) + '\\n')\n",
    "            val_file.flush()\n",
    "            print(\"\\rEpoch: {}  Mean Train Accuracy: {:.4f}% ,Mean Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "                epoch + 1, mean_acc * 100, acc_val * 100, loss_val,\n",
    "                \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "            # Saving the improved model\n",
    "            if loss_val < best_loss_val:\n",
    "                saver.save(sess, CHECKPOINT_PATH_DIR + '/model.tfmodel', global_step=epoch + 1)\n",
    "                best_loss_val = loss_val\n",
    "        train_file.close()\n",
    "        val_file.close()\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    x_test, y_test = load_data(load_type='test')\n",
    "    print('Loaded the test dataset')\n",
    "    test_file = write_progress('test')\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(CHECKPOINT_PATH_DIR)\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print('Model Loaded')\n",
    "        acc_test, loss_test = eval_performance(sess, model, x_test, y_test)\n",
    "        test_file.write(str(acc_test) + ',' + str(loss_test) + '\\n')\n",
    "        test_file.flush()\n",
    "        print('-----------------------------------------------------------------------------')\n",
    "        print(\"Test Set Loss: {0:.4f}, Test Set Accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
    "\n",
    "\n",
    "def reconstruct_sample(model, n_samples=5):\n",
    "    x_test, y_test = load_data(load_type='test')\n",
    "    sample_images, sample_labels = x_test[:BATCH_SIZE], y_test[:BATCH_SIZE]\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(CHECKPOINT_PATH_DIR)\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        feed_dict_samples = {model.X: sample_images, model.Y: sample_labels}\n",
    "        decoder_out, y_predicted = sess.run([model.decoder_output, model.y_predicted],\n",
    "                                       feed_dict=feed_dict_samples)\n",
    "    reconstruction(sample_images, sample_labels, decoder_out, y_predicted, n_samples)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Train the model and evaluate on test set\n",
    "    model = CapsNet()\n",
    "    print (\"Step1: Train\")\n",
    "    train(model)\n",
    "    print(\"Step2: Testing the performance of model on the Test Set\")\n",
    "    test(model)\n",
    "    print (\"Step3: Reconstructing some sample images\")\n",
    "    reconstruct_sample(model,n_samples =3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
